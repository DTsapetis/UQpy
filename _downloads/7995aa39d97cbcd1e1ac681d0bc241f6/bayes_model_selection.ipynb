{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Bayes Model Selection - Regression Models\n\nIn the following we present an example for which the posterior pdf of the parameters, evidences and model probabilities\ncan be computed analytically. We drop the $m_{j}$ subscript when referring to model parameters for simplicity. Three\nmodels are considered (the domain $x$ is fixed and consists in 50 equally spaced points):\n\n\\begin{align}m_{linear}: \\quad y = \u03b8_{0} x + \\epsilon\\end{align}\n\n\\begin{align}m_{quadratic}: \\quad y = \u03b8_{0} x + \u03b8_{1} x^2 + \\epsilon\\end{align}\n\n\\begin{align}m_{cubic}: \\quad y = \u03b8_{0} x + \u03b8_{1} x^2+ \u03b8_{2} x^3 + \\epsilon\\end{align}\n\nAll three models can be written in a compact form as $y=X \u03b8 + \\epsilon$, where $X$ contains the\nnecessary powers of $x$. For all three models, the prior is chosen to be Gaussian,\n$p(\u03b8) = N(\\cdot, \u03b8_{prior}, \\Sigma_{prior})$, and so is the noise\n$\\epsilon \\sim N(\\cdot; 0, \\sigma_{n}^{2} I)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initially we have to import the necessary modules.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import shutil\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom UQpy.sampling.mcmc import MetropolisHastings\nfrom UQpy.inference import BayesModelSelection, BayesParameterEstimation, ComputationalModel\nfrom UQpy.run_model.RunModel import RunModel  # required to run the quadratic model\nfrom UQpy.distributions import Normal, JointIndependent\nfrom scipy.stats import multivariate_normal, norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data from a quadratic function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "param_true = np.array([1.0, 2.0]).reshape(1, -1)\nvar_n = 1\nerror_covariance = var_n * np.eye(50)\nprint(param_true.shape)\n\nfrom UQpy.run_model.model_execution.PythonModel import PythonModel\nm=PythonModel(model_script='local_pfn_models.py', model_object_name='model_quadratic', var_names=['theta_1', 'theta_2'])\nz = RunModel(samples=param_true, model=m)\ndata_clean = z.qoi_list[0].reshape((-1,))\ndata = data_clean + Normal(scale=np.sqrt(var_n)).rvs(nsamples=data_clean.size, random_state=456).reshape((-1,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the models, compute the true values of the evidence.\n\nFor all three models, a Gaussian prior is chosen for the parameters, with mean and covariance matrix of the\nappropriate dimensions. Each model is given prior probability $P(m_{j}) = 1/3$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_names = ['model_linear', 'model_quadratic', 'model_cubic']\nmodel_n_params = [1, 2, 3]\nmodel_prior_means = [[0.], [0., 0.], [0., 0., 0.]]\nmodel_prior_stds = [[10.], [1., 1.], [1., 2., 0.25]]\n\n\ncandidate_models = []\nfor n, model_name in enumerate(model_names):\n    m=PythonModel(model_script='local_pfn_models.py', model_object_name=model_name,)\n    run_model = RunModel(model=m)\n    prior = JointIndependent([Normal(loc=m, scale=std) for m, std in\n                              zip(model_prior_means[n], model_prior_stds[n])])\n    model = ComputationalModel(n_parameters=model_n_params[n],\n                               runmodel_object=run_model, prior=prior,\n                               error_covariance=error_covariance,\n                               name=model_name)\n    candidate_models.append(model)\n\nproposals = [Normal(scale=0.1),\n             JointIndependent([Normal(scale=0.1), Normal(scale=0.1)]),\n             JointIndependent([Normal(scale=0.15), Normal(scale=0.1), Normal(scale=0.05)])]\nnsamples = [2000, 2000, 2000]\nnburn = [1000, 1000, 1000]\njump = [2, 2, 2]\n\n\nsampling_inputs=list()\nestimators = []\nfor i in range(3):\n    sampling = MetropolisHastings(jump=jump[i],\n                                  burn_length=nburn[i],\n                                  proposal=proposals[i],\n                                  seed=model_prior_means[i],\n                                  random_state=123)\n    estimators.append(BayesParameterEstimation(inference_model=candidate_models[i], data=data,\n                                                  sampling_class=sampling))\n\nselection = BayesModelSelection(parameter_estimators=estimators,\n                                prior_probabilities=[1. / 3., 1. / 3., 1. / 3.],\n                                nsamples=nsamples)\n\nsorted_indices = np.argsort(selection.probabilities)[::-1]\nprint('Sorted models:')\nprint([selection.candidate_models[i].name for i in sorted_indices])\nprint('Evidence of sorted models:')\nprint([selection.evidences[i] for i in sorted_indices])\nprint('Posterior probabilities of sorted models:')\nprint([selection.probabilities[i] for i in sorted_indices])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As of version 2, the implementation of BayesModelSelection in UQpy uses the method of the harmonic mean to compute\nthe models' evidence. This method is known to behave quite poorly, in particular it yeidls estimates with large\nvariance. In the problem above, this implementation does not consistently detects that the quadratic model has the\nhighest model probability. Future versions of UQpy will integrate more advanced methods for the estimation of the\nevidence.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, (m, be) in enumerate(zip(selection.candidate_models, selection.bayes_estimators)):\n    # plot prior, true posterior and estimated posterior\n    print('Posterior parameters for model ' + m.name)\n    fig, ax = plt.subplots(1, 3, figsize=(16, 5))\n    for n_p in range(m.n_parameters):\n        domain_plot = np.linspace(min(be.sampler.samples[:, n_p]), max(be.sampler.samples[:, n_p]), 200)\n        ax[n_p].plot(domain_plot, norm.pdf(domain_plot, loc=model_prior_means[i][n_p],\n                                           scale=model_prior_stds[i][n_p]),\n                     label='prior', color='green', linestyle='--')\n        ax[n_p].plot(domain_plot, norm.pdf(domain_plot, loc=model_posterior_means[i][n_p],\n                                           scale=model_posterior_stds[i][n_p]),\n                     label='true posterior', color='red', linestyle='-')\n        ax[n_p].hist(be.sampler.samples[:, n_p], density=True, bins=30, label='estimated posterior MCMC')\n        ax[n_p].legend()\n    plt.show()\n\nshutil.rmtree(z.model_dir)\nshutil.rmtree(candidate_models[0].runmodel_object.model_dir)\nshutil.rmtree(candidate_models[1].runmodel_object.model_dir)\nshutil.rmtree(candidate_models[2].runmodel_object.model_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
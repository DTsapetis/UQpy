{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hyperbolic truncation and Least Angle Regression\n\nIn this example, we approximate the well-known Ishigami function with a total-degree Polynomial Chaos Expansion further\nreduced by hyperbolic truncation. In order to reduce the number of basis functions, we use the best-model selection\nalgorithm based on Least Angle Regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import necessary libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom UQpy.distributions import Uniform, JointIndependent\nfrom UQpy.surrogates import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then define the Ishigami function, which reads:\n\n\\begin{align}f(x_1, x_2, x_3) = \\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1)\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ishigami(xx):\n    \"\"\"Ishigami function\"\"\"\n    a = 7\n    b = 0.1\n    term1 = np.sin(xx[0])\n    term2 = a * np.sin(xx[1]) ** 2\n    term3 = b * xx[2] ** 4 * np.sin(xx[0])\n    return term1 + term2 + term3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Ishigami function has three independent random inputs, which are uniformly distributed in\ninterval $[-\\pi, \\pi]$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# input distributions\ndist1 = Uniform(loc=-np.pi, scale=2 * np.pi)\ndist2 = Uniform(loc=-np.pi, scale=2 * np.pi)\ndist3 = Uniform(loc=-np.pi, scale=2 * np.pi)\nmarg = [dist1, dist2, dist3]\njoint = JointIndependent(marginals=marg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define our complete PCE, which will be further used for the best model selection algorithm.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We must now select a polynomial basis. Here we opt for a total-degree (TD) basis, such that the univariate\npolynomials have a maximum degree equal to $P$ and all multivariate polynomial have a total-degree\n(sum of degrees of corresponding univariate polynomials) at most equal to $P$. The size of the basis\nis then given by\n\n\\begin{align}\\frac{(N+P)!}{N! P!}\\end{align}\n\nwhere $N$ is the number of random inputs (here, $N=3$).\n\nNote that the size of the basis is highly dependent both on $N$ and $P$. It is generally advisable\nthat the experimental design has $2-10$ times more data points than the number of PCE polynomials. This might\nlead to curse of dimensionality and thus we will utilize the best model selection algorithm based on\nLeast Angle Regression.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# maximum polynomial degree\nP = 15\n# construct total-degree polynomial basis\npolynomial_basis = TotalDegreeBasis(joint, P)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We must now compute the PCE coefficients. For that we first need a training sample of input random variable\nrealizations and the corresponding model outputs. These two data sets form what is also known as an\n''experimental design''. In case of adaptive construction of PCE by the best model selection algorithm, size of\nED is given apriori and the most suitable basis functions are adaptively selected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create training data\nsample_size = 500\nprint('Size of experimental design:', sample_size)\n\n# realizations of random inputs\nxx_train = joint.rvs(sample_size)\n# corresponding model outputs\nyy_train = np.array([ishigami(x) for x in xx_train])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now fit the PCE coefficients by solving a regression problem. Here we opt for the :code:`_np.linalg.lstsq_` method,\nwhich is based on the :code:`_dgelsd_` solver of LAPACK. This original PCE class will be used for further selection of\nthe best basis functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# fit model\nleast_squares = LeastSquareRegression()\npce = PolynomialChaosExpansion(polynomial_basis=polynomial_basis, regression_method=least_squares)\npce.fit(xx_train, yy_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have created the PCE containing all basis functions generated by TD algorithm, it is possible to reduce\nthe number of basis functions by LAR algorithm. The best model selection algorithm in UQPy is based on results\nof LAR adding basis functions to active set one-by-one until the target accuracy is obtained. Approximation\nerror is measured by leave-one-out error $Q^2$ on given ED in $[0,1]$. Target error represents the target\naccuracy measured by $Q^2$.\n\nNote that if the target error is too high (close to 1), there is a risk of over-fitting. Therefore, we must check\nthe over-fitting by empirical rule: if the three steps of LAR in row lead to decreasing accuracy - stop the\nalgorithm. It is recommended to always check the over-fitting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# check the size of the basis\nprint('Size of the full set of PCE basis:', polynomial_basis.polynomials_number)\n\ntarget_error = 1\nCheckOverfitting = True\npceLAR = polynomial_chaos.regressions.LeastAngleRegression.model_selection(pce, target_error, CheckOverfitting)\n\nprint('Size of the LAR PCE basis:', pceLAR.polynomial_basis.polynomials_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By simply post-processing the PCE's terms, we are able to get estimates regarding the mean and standard deviation of\nthe model output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean_est, var_est = pceLAR.get_moments(higher=False)\nprint('PCE mean estimate:', mean_est)\nprint('PCE variance estimate:', var_est)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to obtain skewness and kurtosis (3rd and 4th moments), though it might be computationally demanding\nfor high $N$ and $P$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean_est, var_est, skew_est, kurt_est = pceLAR.get_moments(True)\nprint('PCE mean estimate:', mean_est)\nprint('PCE variance estimate:', var_est)\nprint('PCE skewness estimate:', skew_est)\nprint('PCE kurtosis estimate:', kurt_est)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly to the statistical moments, we can very simply estimate the Sobol sensitivity indices, which quantify the\nimportance of the input random variables in terms of impact on the model output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from UQpy.sensitivity import *\n\npce_sensitivity = PceSensitivity(pceLAR)\npce_sensitivity.run()\nsobol_first = pce_sensitivity.first_order_indices\nsobol_total = pce_sensitivity.total_order_indices\nprint('First-order Sobol indices:')\nprint(sobol_first)\nprint('Total-order Sobol indices:')\nprint(sobol_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The accuracy of PCE is typically measured by leave-one-out error $Q^2$ on given ED. Moreover, we will test that\nalso by computing the mean absolute error (MAE) between the PCE's predictions and the true model outputs, given a\nvalidation sample of $10^5$ data points.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples_val = 100000\nxx_val = joint.rvs(n_samples_val)\nyy_val = np.array([ishigami(x) for x in xx_val])\n\nyy_val_pce = pceLAR.predict(xx_val).flatten()\nerrors = np.abs(yy_val.flatten() - yy_val_pce)\nMAE = (np.linalg.norm(errors, 1) / n_samples_val)\n\nprint('Mean absolute error:', MAE)\nprint('Leave-one-out cross validation on ED:', pceLAR.leaveoneout_error())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the comparison, we can check results of PCE solved by OLS without the model selection algorithm. Note that, it is\nnecessary to use 2\u221210 times more data points than the number of PCE polynomials.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# validation data sets\nnp.random.seed(999)  # fix random seed for reproducibility\nn_samples_val = 100000\nxx_val = joint.rvs(n_samples_val)\nyy_val = np.array([ishigami(x) for x in xx_val])\n\nmae = []  # to hold MAE for increasing polynomial degree\nfor degree in range(16):\n    # define PCE\n    polynomial_basis = TotalDegreeBasis(joint, degree)\n    least_squares = LeastSquareRegression()\n    pce_metamodel = PolynomialChaosExpansion(polynomial_basis=polynomial_basis, regression_method=least_squares)\n\n    # create training data\n    np.random.seed(1)  # fix random seed for reproducibility\n    sample_size = int(pce_metamodel.polynomials_number * 5)\n    xx_train = joint.rvs(sample_size)\n    yy_train = np.array([ishigami(x) for x in xx_train])\n\n    # fit PCE coefficients\n    pce_metamodel.fit(xx_train, yy_train)\n\n    # compute mean absolute validation error\n    yy_val_pce = pce_metamodel.predict(xx_val).flatten()\n    errors = np.abs(yy_val.flatten() - yy_val_pce)\n    mae.append(np.linalg.norm(errors, 1) / n_samples_val)\n    print('Size of ED:', sample_size)\n    print('Polynomial degree:', degree)\n    print('Mean absolute error:', mae[-1])\n    print(' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case of high-dimensional input and/or high $P$ it is also beneficial to reduce the TD basis set by hyperbolic\ntruncation. The hyperbolic truncation reduces higher-order interaction terms in dependence to parameter $q$ in\ninterval $(0,1)$. The set of multi indices $\\alpha$ is reduced as follows:\n\n$\\alpha\\in \\mathbb{N}^{N}: || \\boldsymbol{\\alpha}||_q \\equiv \\Big( \\sum_{i=1}^{N} \\alpha_i^q \\Big)^{1/q} \\leq P$\n\nNote that $q=1$ leads to full TD set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Size of the full set of PCE basis:', TotalDegreeBasis(joint, P).polynomials_number)\nq = 0.8\npolynomial_basis_hyperbolic = TotalDegreeBasis(joint, P, q)\n# check the size of the basis\nprint('Size of the hyperbolic full set of PCE basis:', polynomial_basis_hyperbolic.polynomials_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reduction of the full set size significantly reduces the necessary number of data points in ED for non-adaptive\nPCE. However, it is suitable only for mathematical models without significant higher-order interaction terms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pce = PolynomialChaosExpansion(polynomial_basis=polynomial_basis_hyperbolic, regression_method=least_squares)\npce.fit(xx_train, yy_train)\nyy_val_pce = pce.predict(xx_val).flatten()\nerrors = np.abs(yy_val.flatten() - yy_val_pce)\nMAE = (np.linalg.norm(errors, 1) / n_samples_val)\n\nprint('Mean absolute error:', MAE)\nprint('Leave-one-out cross validation on ED:', pce.leaveoneout_error())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training a Bayesian neural network\n\nIn this example, we train a Bayesian neural network to learn the function $f(x)=x^2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we have to import the necessary modules.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Default imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport UQpy.scientific_machine_learning as sml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the network architecture using the ``nn.Sequential`` object\nand instantiate the ``BayesianNeuralNetwork``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "width = 5\nnetwork = nn.Sequential(\n    sml.BayesianLinear(1, width),\n    nn.ReLU(),\n    sml.BayesianLinear(width, width),\n    nn.ReLU(),\n    sml.BayesianLinear(width, 1),\n)\nmodel = sml.FeedForwardNeuralNetwork(network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the neural network defined, we turn our attention to the training data.\nWe want to learn the function $f(x)=x^2$ and define the training data using the\npytorch Dataset and Dataloader.\n\nFor more information on defining the training data,\nsee the pytorch documentation at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class QuadraticDataset(Dataset):\n    def __init__(self, n_samples=200):\n        self.n_samples = n_samples\n        self.x = torch.linspace(-5, 5, n_samples, dtype=torch.float).reshape(-1, 1)\n        self.y = self.x**2\n\n    def __len__(self):\n        return self.n_samples\n\n    def __getitem__(self, item):\n        return self.x[item], self.y[item]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we continue with training the network, let's get the initial prediction of the neural network on the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initial_prediction = model(QuadraticDataset().x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far we have the neural network and training data. The ``BBBTrainer`` combines the two along with a\npytorch optimization algorithm to learn the network parameters.\nWe instantiate the ``BBBTrainer``, train the network, then print the initial and final loss alongside a model summary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ntrain_data = DataLoader(QuadraticDataset(), batch_size=20, shuffle=True)\ntrainer = sml.BBBTrainer(model, optimizer)\nprint(\"Starting Training...\", end=\"\")\ntrainer.run(train_data=train_data, epochs=300, beta=1e-6, num_samples=10)\nprint(\"done\")\n\nprint(\"Initial loss:\", trainer.history[\"train_loss\"][0])\nprint(\"Final loss:\", trainer.history[\"train_loss\"][-1])\nmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We compare the initial and final predictions and plot the loss history using matplotlib.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = QuadraticDataset().x\ny = QuadraticDataset().y\nmodel.train(False)\nmodel.sample(False)\nfinal_prediction = model(x)\nfig, ax = plt.subplots()\nax.plot(\n    x.detach().numpy(),\n    initial_prediction.detach().numpy(),\n    label=\"Initial Prediction\",\n    color=\"tab:blue\",\n)\nax.plot(\n    x.detach().numpy(),\n    final_prediction.detach().numpy(),\n    label=\"Final Prediction\",\n    color=\"tab:orange\",\n)\nax.plot(\n    x.detach().numpy(),\n    y.detach().numpy(),\n    label=\"Exact\",\n    color=\"black\",\n    linestyle=\"dashed\",\n)\nax.set_title(\"Initial and Final NN Predictions\")\nax.set(xlabel=\"x\", ylabel=\"f(x)\")\nax.legend()\n\ntrain_loss = trainer.history[\"train_loss\"].detach().numpy()\nfig, ax = plt.subplots()\nax.plot(train_loss)\nax.set_title(\"Bayes By Backpropagation Training Loss\")\nax.set(xlabel=\"Epoch\", ylabel=\"Loss\")\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Bayesian neural network is a probabilistic model. Each of its parameters, in this case weights and biases,\nare governed by Gaussian distributions. We can get a deterministic output from the BNN by setting\n``model.sample(False)``, which sets each parameter to the mean of its distribution.\n\nWe can obtain error bars on model's output by sampling the parameters from their governing distribution.\nThis is done by setting ``model.sample(True)`` and computing the forward model evaluation many times,\nthen computing the sample variance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.sample(False)\nprint(\"BNN is deterministic:\", model.is_deterministic())\nmean = model(x)\n\nmodel.sample(True)\nprint(\"BNN is deterministic:\", model.is_deterministic())\nn = 10_000\nsamples = torch.zeros(len(x), n)\nfor i in range(n):\n    samples[:, i] = model(x).squeeze()\nvariance = torch.var(samples, dim=1)\nstandard_deviation = torch.sqrt(variance)\n\nx_plot = x.squeeze().detach().numpy()\nmu = mean.squeeze().detach().numpy()\nsigma = standard_deviation.squeeze().detach().numpy()\nfig, ax = plt.subplots()\nax.plot(x_plot, mu, label=\"$\\mu$\")\nax.plot(x_plot, y.detach().numpy(), label=\"Exact\", color=\"black\", linestyle=\"dashed\")\nax.fill_between(\n    x_plot, mu - (3 * sigma), mu + (3 * sigma), label=\"$\\mu \\pm 3\\sigma$,\", alpha=0.3\n)\nax.set_title(\"Bayesian Neural Network $\\mu \\pm 3\\sigma$\")\nax.set(xlabel=\"x\", ylabel=\"f(x)\")\nax.legend()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Learning the Burgers' Operator with HMC\n\nIn this example, we train a Fourier Neural Operator (FNO) using the implementation of Hamiltonian Monte Carlo (HMC)\nprovided by the Hamiltorch (https://adamcobb.github.io/journal/hamiltorch.html) library.\nOur FNO learns the mapping $y(x, 0) \\mapsto y(x, 0.5)$ where $y(x,t)$ is the solution to the\nBurgers' equation given by\n\n\\begin{align}\\frac{\\partial}{\\partial t}u(x, t) + u(x, t) \\frac{\\partial}{\\partial x} u(x, t) = \\nu \\frac{\\partial^2}{\\partial x^2} u(x,t)\\end{align}\n\nThis example is adapted from this Hamiltorch example (https://github.com/AdamCobb/hamiltorch/blob/master/notebooks/hamiltorch_Bayesian_NN_example.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We learn the mapping from the initial condition $y(x, 0)$ to a point at time $t=0.5$.\nThe training data for this example was computed separately and is available on GitHub.\n\nFirst, import the necessary modules.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport hamiltorch\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport UQpy.scientific_machine_learning as sml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we define the architecture of our neural operator as subclasses of PyTorch objects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class FourierNeuralOperator(nn.Module):\n\n    def __init__(self):\n        \"\"\"Construct a Bayesian FNO\n\n        The lifting layers a single, deterministic, and fully connected linear layer.\n        There are four Bayesian Fourier layers performing a 1d Fourier\n        transform with ReLU activation functions and batch normalization in between.\n        The projection layers are also a single, deterministic, and fully connected linear layer.\n        \"\"\"\n        super().__init__()\n        modes = 16\n        width = 8\n        self.lifting_layers = nn.Sequential(\n            sml.Permutation((0, 2, 1)),\n            nn.Linear(1, width),\n            sml.Permutation((0, 2, 1)),\n        )\n        self.fourier_blocks = nn.Sequential(\n            sml.Fourier1d(width, modes),\n            nn.ReLU(),\n            sml.Fourier1d(width, modes),\n            nn.ReLU(),\n            sml.Fourier1d(width, modes),\n            nn.ReLU(),\n            sml.Fourier1d(width, modes),\n        )\n        self.projection_layers = nn.Sequential(\n            sml.Permutation((0, 2, 1)),\n            nn.Linear(width, 1),\n            sml.Permutation((0, 2, 1)),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward computational call of the FNO.\n\n        Apply the lifting layers, then theFourier blocks, and finally projection layers.\n\n        :param x: Tensor of shape :math:`(N, C, L)`\n        :return: Tensor of shape :math:`(N, C, L)`\n        \"\"\"\n        x = self.lifting_layers(x)\n        x = self.fourier_blocks(x)\n        x = self.projection_layers(x)\n        return x\n\n\nmodel = FourierNeuralOperator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training and testing data was created by solving the Burgers' equation using numerical integration.\nThe dataset used here are available on our GitHub.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_train = torch.load(\"initial_conditions_train.pt\")\ny_train = torch.load(\"burgers_solutions_train.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we pass our model and the data off the Hamiltorch for the training.\nNote that Hamiltorch does not modify the parameters of the ``model`` object.\nRather, it provides a list of parameters that have been sampled from the posterior distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set hyperparameters for network\ntau_list = []\ntau = 100.0\nfor w in model.parameters():\n    tau_list.append(tau)\ntau_list = torch.tensor(tau_list)\nstep_size = 1e-4\nnum_steps_per_sample = 100\nparams_init = hamiltorch.util.flatten(model).clone()\n\n# generate the HMC samples\nparams_hmc = hamiltorch.sample_model(\n    model,\n    x_train,\n    y_train,\n    params_init=params_init,\n    num_samples=100,\n    step_size=step_size,\n    num_steps_per_sample=num_steps_per_sample,\n    tau_list=tau_list,\n    model_loss=nn.MSELoss(reduction=\"sum\"),\n)\n# optional, save the HMC  parameters\n# torch.save(params_hmc, \"params_hmc.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's the Fourier neural operator trained with HMC!\nThe rest of this notebook visualizes the predictions from the sampled parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# optional, load the HMC parameters. `params_hmc.pt` is available on our GitHub\nparams_hmc = torch.load(\"params_hmc.pt\")\n\n# Evaluate the HMC samples on testing data\nx_test = torch.load(\"initial_conditions_test.pt\")\ny_test = torch.load(\"burgers_solutions_test.pt\")\npredictions, log_prob_list = hamiltorch.predict_model(\n    model,\n    x=x_test[0:1],\n    y=y_test[0:1],\n    samples=params_hmc[:],\n    model_loss=nn.MSELoss(reduction=\"sum\"),\n    tau_list=tau_list,\n)\n\n# plot the results\npredictions = predictions.squeeze()\nmean_prediction = torch.mean(predictions, dim=0)\nspacial_x = torch.linspace(0, 1, 256)\nfig, ax = plt.subplots()\nax.plot(spacial_x, y_test[0].squeeze(), color=\"tab:blue\")\nax.plot(spacial_x, mean_prediction, color=\"black\")\nax.plot(spacial_x, predictions.T, color=\"gray\", alpha=0.1, zorder=1)\nax.legend([\"y\", \"Mean Prediction\", \"Ensemble Predictions\"])\nax.set_title(\"HMC Predictions\")\nax.set(xlabel=\"x\", ylabel=\"y(x, 0.5)\")\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sequential Tempering for Bayesian Inference and Reliability analyses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The general framework: one wants to sample from a distribution of the form\n\n\\begin{align}p_1 \\left( x \\right) = \\frac{q_1 \\left( x \\right)p_0 \\left( x \\right)}{Z_1}\\end{align}\n\n\nwhere $q_1 \\left( x \\right)$ and $p_0 \\left( x \\right)$ can be evaluated; and potentially estimate the\nconstant $Z_1 = \\int q_1 \\left( x \\right)p_0 \\left( x \\right) dx$.\n\nSequential tempering introduces a sequence of intermediate distributions:\n\n\\begin{align}p_{\\beta_j} \\left( x \\right) \\propto q \\left( x, \\beta_j \\right)p_0 \\left( x \\right)\\end{align}\n\nfor values of $\\beta_j$ in $[0, 1]$. The algorithm starts with $\\beta_0 = 0$, which samples\nfrom the reference distribution $p_0$, and ends for some $j = m$ such that $\\beta_m = 1$, sampling\nfrom the target. First, a set of sample points is generated from $p_0 = p_{\\beta_0}$, and then these are\nresampled according to some weights $w_0$ such that after resampling the points follow $p_{\\beta_1}$.\nThis procedure of resampling is carried out at each intermediate level $j$ - resampling the points distributed\nas $p_{\\beta_{j}}$ according to weights $w_{j}$ such that after resampling, the points are distributed\naccording to $p_{\\beta_{j+1}}$. As the points are sequentially resampled to follow each intermediate\ndistribution, eventually they are resampled from $p_{\\beta_{m-1}}$ to follow $p_{\\beta_{m}} = p_1$.\n\nThe weights are calculated as\n\n\\begin{align}w_j = \\frac{q \\left( x, \\beta_{j+1} \\right)}{q \\left( x, \\beta_j \\right)}\\end{align}\n\nThe normalizing constant is calculated during the generation of samples, as\n\n\\begin{align}Z_1 = \\prod_{j = 0}^{m-1} \\left\\{ \\frac{\\sum_{i = 1}^{N_j} w_j}{N_j} \\right\\}\\end{align}\n\nwhere $N_j$ is the number of sample points generated from the intermediate distribution $p_{\\beta_j}$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian Inference\n\nIn the Bayesian setting, $p_0$ is the prior, and $q \\left( x, \\beta_j \\right) = \\mathcal{L}\\left( data, x \\right) ^{\\beta_j}$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from UQpy.run_model import RunModel, PythonModel\nimport numpy as np\nfrom UQpy.distributions import Uniform, Normal, JointIndependent, MultivariateNormal\nfrom UQpy.sampling import SequentialTemperingMCMC\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, norm, uniform\nfrom UQpy.sampling.mcmc import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def likelihood(x, b):\n    mu1 = np.array([1., 1.])\n    mu2 = -0.8 * np.ones(2)\n    w1 = 0.5\n    # Width of 0.1 in each dimension\n    sigma1 = np.diag([0.02, 0.05])\n    sigma2 = np.diag([0.05, 0.02])\n\n    # Posterior is a mixture of two gaussians\n    like = np.exp(np.logaddexp(np.log(w1) + multivariate_normal.logpdf(x=x, mean=mu1, cov=sigma1),\n                               np.log(1. - w1) + multivariate_normal.logpdf(x=x, mean=mu2, cov=sigma2)))\n    return like ** b\n\n\nprior = JointIndependent(marginals=[Uniform(loc=-2.0, scale=4.0), Uniform(loc=-2.0, scale=4.0)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# estimate evidence\ndef estimate_evidence_from_prior_samples(size):\n    samples = -2. + 4 * np.random.uniform(size=size * 2).reshape((size, 2))\n    return np.mean(likelihood(samples, 1.0))\n\n\ndef func_integration(x1, x2):\n    x = np.array([x1, x2]).reshape((1, 2))\n    return likelihood(x, 1.0) * (1. / 4) ** 2\n\n\ndef estimate_evidence_from_quadrature():\n    from scipy.integrate import dblquad\n    ev = dblquad(func=func_integration, a=-2, b=2, gfun=lambda x: -2, hfun=lambda x: 2)\n    return ev\n\n\nx = np.arange(-2, 2, 0.02)\ny = np.arange(-2, 2, 0.02)\nxx, yy = np.meshgrid(x, y)\nz = likelihood(np.concatenate([xx.reshape((-1, 1)), yy.reshape((-1, 1))], axis=-1), 1.0)\nh = plt.contourf(x, y, z.reshape(xx.shape))\nplt.title('Likelihood')\nplt.axis('equal')\nplt.show()\n\n# for nMC in [50000, 100000, 500000, 1000000]:\n#    print('Evidence = {}'.format(estimate_evidence_from_prior_samples(nMC)))\nprint('Evidence computed analytically = {}'.format(estimate_evidence_from_quadrature()[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampler = MetropolisHastings(dimension=2, n_chains=20)\ntest = SequentialTemperingMCMC(pdf_intermediate=likelihood,\n                               distribution_reference=prior,\n                               save_intermediate_samples=True,\n                               percentage_resampling=10,\n                               sampler=sampler,\n                               nsamples=4000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Normalizing Constant = ' + str(test.evidence))\nprint('Tempering Parameters = ' + str(test.tempering_parameters))\n\nplt.figure()\nplt.scatter(test.intermediate_samples[0][:, 0], test.intermediate_samples[0][:, 1])\nplt.title(r'$\\beta = $' + str(test.tempering_parameters[0]))\nplt.show()\n\nplt.figure()\nplt.scatter(test.intermediate_samples[2][:, 0], test.intermediate_samples[2][:, 1])\nplt.title(r'$\\beta = $' + str(test.tempering_parameters[2]))\nplt.show()\n\nplt.figure()\nplt.scatter(test.samples[:, 0], test.samples[:, 1])\nplt.title(r'$\\beta = $' + str(test.tempering_parameters[-1]))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reliability\n\nIn the reliability context, $p_0$ is the pdf of the parameters, and\n\n\\begin{align}q \\left( x, \\beta_j \\right) = I_{\\beta_j} \\left( x \\right) = \\frac{1}{1 + \\exp{\\left( \\frac{G \\left( x \\right)}{\\frac{1}{\\beta_j} - 1} \\right)}}\\end{align}\n\nwhere $G \\left( x \\right)$ is the performance function, negative if the system fails, and $I_{\\beta_j} \\left( x \\right)$ are smoothed versions of the indicator function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n\n\ndef indic_sigmoid(y, beta):\n    return 1. / (1. + np.exp(y / (1. / beta - 1.)))\n\n\nfig, ax = plt.subplots(figsize=(4, 3.5))\nys = np.linspace(-5, 5, 100)\nfor i, s in enumerate(1. / np.array([1.01, 1.25, 2., 4., 70.])):\n    ax.plot(ys, indic_sigmoid(y=ys, beta=s), label=r'$\\beta={:.2f}$'.format(s), color='blue', alpha=1. - i / 6)\nax.set_xlabel(r'$y=g(\\theta)$', fontsize=13)\nax.set_ylabel(r'$q_{\\beta}(\\theta)=I_{\\beta}(y)$', fontsize=13)\n# ax.set_title(r'Smooth versions of the indicator function', fontsize=14)\nax.legend(fontsize=8.5)\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "beta = 2  # Specified Reliability Index\nrho = 0.7  # Specified Correlation\ndim = 2  # Dimension\n\n# Define the correlation matrix\nC = np.ones((dim, dim)) * rho\nnp.fill_diagonal(C, 1)\nprint(C)\n\n# Print information related to the true probability of failure\ne, v = np.linalg.eig(np.asarray(C))\nbeff = np.sqrt(np.max(e)) * beta\nprint(beff)\nfrom scipy.stats import norm\n\npf_true = norm.cdf(-beta)\nprint('True pf={}'.format(pf_true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def estimate_Pf_0(samples, model_values):\n    mask = model_values <= 0\n    return np.sum(mask) / len(mask)\n\n\nmodel = RunModel(model=PythonModel(model_script='local_reliability_funcs.py', model_object_name=\"correlated_gaussian\",\n                                   b_eff=beff, d=dim))\nsamples = MultivariateNormal(mean=np.zeros((2,)), cov=np.array([[1, 0.7], [0.7, 1]])).rvs(nsamples=20000)\nmodel.run(samples=samples, append_samples=False)\nmodel_values = np.array(model.qoi_list)\n\nprint('Prob. failure (MC) = {}'.format(estimate_Pf_0(samples, model_values)))\n\nfig, ax = plt.subplots(figsize=(4, 3.5))\nmask = np.squeeze(model_values <= 0)\nax.scatter(samples[mask, 0], samples[mask, 1], color='red', label='fail', alpha=0.5, marker='d')\nax.scatter(samples[~mask, 0], samples[~mask, 1], color='blue', label='safe', alpha=0.5)\nplt.axis('equal')\n# plt.title('Failure domain for reliability problem', fontsize=14)\nplt.xlabel(r'$\\theta_{1}$', fontsize=13)\nplt.ylabel(r'$\\theta_{2}$', fontsize=13)\nax.legend(fontsize=13)\nfig.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def indic_sigmoid(y, b):\n    return 1.0 / (1.0 + np.exp((y * b) / (1.0 - b)))\n\n\ndef factor_param(x, b):\n    model.run(samples=x, append_samples=False)\n    G_values = np.array(model.qoi_list)\n    return np.squeeze(indic_sigmoid(G_values, b))\n\n\nprior = MultivariateNormal(mean=np.zeros((2,)), cov=C)\n\nsampler = MetropolisHastings(dimension=2, n_chains=20)\ntest = SequentialTemperingMCMC(pdf_intermediate=factor_param,\n                               distribution_reference=prior,\n                               save_intermediate_samples=True,\n                               percentage_resampling=10,\n                               random_state=960242069,\n                               sampler=sampler,\n                               nsamples=3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Estimated Probability of Failure = ' + str(test.evidence))\nprint('Tempering Parameters = ' + str(test.tempering_parameters))\n\nplt.figure()\nplt.scatter(test.intermediate_samples[0][:, 0], test.intermediate_samples[0][:, 1])\nplt.title(r'$\\beta = $' + str(test.tempering_parameters[0]))\nplt.show()\n\nplt.figure()\nplt.scatter(test.intermediate_samples[2][:, 0], test.intermediate_samples[2][:, 1])\nplt.title(r'$\\beta = $' + str(test.tempering_parameters[2]))\nplt.show()\n\nplt.figure()\nplt.scatter(test.samples[:, 0], test.samples[:, 1])\nplt.title(r'$\\beta = $' + str(test.tempering_parameters[-1]))\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
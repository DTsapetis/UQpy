{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Parallel Tempering for Bayesian Inference and Reliability analyses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The general framework: one wants to sample from a distribution of the form\n\n\\begin{align}p_{1}(x)=\\dfrac{q_1(x)p_{0}(x)}{Z_{1}}\\end{align}\n\nwhere $q_{1}(x)$ and $p_{0}(x)$ can be evaluated; and potentially estimate the constant\n$Z_{1}=\\int{q_{1}(x) p_{0}(x)dx}$. Parallel tempering introduces a sequence of intermediate distributions:\n\n\\begin{align}p_{\\beta}(x) \\propto q(x, \\beta) p_{0}(x)\\end{align}\n\nfor values of $\\beta$ in [0, 1] (note: $\\beta$ is $1/T$ where $T$ is often referred as the\ntemperature). Setting $\\beta=1$ equates sampling from the target, while $\\beta \\rightarrow 0$ samples from\nthe reference distribution $p_{0}$. Periodically during the run, the different temperatures swap members of\ntheir ensemble in a way that preserves detailed balance. The chains closer to the reference chain (hot chains) can\nsample from regions that have low probability under the target and thus allow a better exploration of the parameter\nspace, while the cold chains can better explore the regions of high likelihood.\n\nThe normalizing constant $Z_{1}$ is estimated via thermodynamic integration:\n\n\\begin{align}\\ln{Z_{\\beta=1}} = \\ln{Z_{\\beta=0}} + \\int_{0}^{1} E_{p_{\\beta}} \\left[ \\frac{\\partial \\ln{q_{\\beta}(x)}}{\\partial \\beta} \\right] d\\beta = \\ln{Z_{\\beta=0}} + \\int_{0}^{1} E_{p_{\\beta}} \\left[ U_{\\beta}(x) \\right] d\\beta\\end{align}\n\nwhere $\\ln{Z_{\\beta=0}}=\\int{q_{\\beta=0}(x) p_{0}(x)dx}$ can be determined by simple MC sampling since\n$q_{\\beta=0}(x)$ is close to the reference distribution $p_{0}$. The function\n$U_{\\beta}(x)=\\frac{\\partial \\ln{q_{\\beta}(x)}}{\\partial \\beta}$ is called the potential, and can be evaluated\nusing posterior samples from $p_{\\beta}(x)$.\n\nIn the code, the user must define:\n- a function to evaluate the reference distribution $p_{0}(x)$,\n- a function to evaluate the intermediate factor $q(x, \\beta)$ (function that takes in two inputs: x and\n$\\beta$),\n- if evaluation of $Z_{1}$ is of interest, a function that evaluates the potential $U_{\\beta}(x)$, from\nevaluations of $\\ln{(x, \\beta)}$ which are saved during the MCMC run for the various chains (different\n$\\beta$ values).\n\nBayesian inference\n\nIn the Bayesian setting, $p_{0}$ is the prior and, given a likelihood $L(data; x)$:\n\n\\begin{align}q_{T}(x) = L(data; x) ^{\\beta}\\end{align}\n\nThen for the model evidence:\n\n\\begin{align}U_{\\beta}(x) = \\ln{L(data; x)}\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom UQpy.run_model import RunModel, PythonModel\nfrom UQpy.distributions import MultivariateNormal, JointIndependent, Normal, Uniform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal, norm, uniform\n\n# bimodal posterior\nmu1 = np.array([1., 1.])\nmu2 = -0.8 * np.ones(2)\nw1 = 0.5\n# Width of 0.1 in each dimension\nsigma1 = np.diag([0.02, 0.05])\nsigma2 = np.diag([0.05, 0.02])\n\n# define prior, likelihood and target (posterior)\nprior_distribution = JointIndependent(marginals=[Uniform(loc=-2, scale=4), Uniform(loc=-2, scale=4)])\n\n\ndef log_likelihood(x):\n    # Posterior is a mixture of two gaussians\n    return np.logaddexp(np.log(w1) + multivariate_normal.logpdf(x=x, mean=mu1, cov=sigma1),\n                        np.log(1. - w1) + multivariate_normal.logpdf(x=x, mean=mu2, cov=sigma2))\n\n\ndef log_target(x):\n    return log_likelihood(x) + prior_distribution.log_pdf(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# estimate evidence\ndef estimate_evidence_from_prior_samples(size):\n    samples = -2. + 4 * np.random.uniform(size=size * 2).reshape((size, 2))\n    return np.mean(np.exp(log_likelihood(samples)))\n\n\ndef func_integration(x1, x2):\n    x = np.array([x1, x2]).reshape((1, 2))\n    return np.exp(log_likelihood(x)) * (1. / 4) ** 2\n\n\ndef estimate_evidence_from_quadrature():\n    from scipy.integrate import dblquad\n    ev = dblquad(func=func_integration, a=-2, b=2, gfun=lambda x: -2, hfun=lambda x: 2)\n    return ev\n\n\nx = np.arange(-2, 2, 0.02)\ny = np.arange(-2, 2, 0.02)\nxx, yy = np.meshgrid(x, y)\nz = np.exp(log_likelihood(np.concatenate([xx.reshape((-1, 1)), yy.reshape((-1, 1))], axis=-1)))\nh = plt.contourf(x, y, z.reshape(xx.shape))\nplt.title('Likelihood')\nplt.axis('equal')\nplt.show()\n\nprint('Evidence computed analytically = {}'.format(estimate_evidence_from_quadrature()[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from UQpy.sampling.mcmc import MetropolisHastings\n\nseed = -2. + 4. * np.random.rand(100, 2)\nmcmc0 = MetropolisHastings(log_pdf_target=log_target, burn_length=100, jump=3, seed=list(seed), dimension=2,\n                           random_state=123, save_log_pdf=True)\nmcmc0.run(nsamples_per_chain=200)\n\nprint(mcmc0.samples.shape)\nfig, ax = plt.subplots(ncols=1, figsize=(6, 4))\nax.scatter(mcmc0.samples[:, 0], mcmc0.samples[:, 1], alpha=0.5)\nax.set_xlim([-2, 2])\nax.set_ylim([-2, 2])\nplt.show()\n\n\ndef estimate_evidence_from_posterior_samples(log_posterior_values, posterior_samples):\n    log_like = log_likelihood(posterior_samples)  # log_posterior_values - log_prior(posterior_samples)\n    ev = 1. / np.mean(1. / np.exp(log_like))\n    return ev\n\n\nevidence = estimate_evidence_from_posterior_samples(\n    log_posterior_values=mcmc0.log_pdf_values, posterior_samples=mcmc0.samples)\nprint('Estimated evidence by HM={}'.format(evidence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def log_intermediate(x, temper_param):\n    return temper_param * log_likelihood(x)  # + (1. - 1. / temperature) * log_prior(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from UQpy.sampling import MetropolisHastings, ParallelTemperingMCMC\n\nseed = -2. + 4. * np.random.rand(5, 2)\nbetas = [1. / np.sqrt(2.) ** i for i in range(20 - 1, -1, -1)]\nprint(len(betas))\nprint(betas)\n\nsamplers = [MetropolisHastings(burn_length=100, jump=3, seed=list(seed), dimension=2) for _ in range(len(betas))]\nmcmc = ParallelTemperingMCMC(log_pdf_intermediate=log_intermediate,\n                             distribution_reference=prior_distribution,\n                             n_iterations_between_sweeps=10,\n                             tempering_parameters=betas,\n                             random_state=123,\n                             save_log_pdf=True, samplers=samplers)\n\nmcmc.run(nsamples_per_chain=200)\nprint(mcmc.samples.shape)\nprint(mcmc.mcmc_samplers[-1].samples.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the intermediate samples can be accessed via the mcmc_samplers.samples attributes or\n# directly via the intermediate_samples attribute\nfig, ax = plt.subplots(ncols=3, figsize=(12, 3.5))\nfor j, ind in enumerate([0, -6, -1]):\n    ax[j].scatter(mcmc.mcmc_samplers[ind].samples[:, 0], mcmc.mcmc_samplers[ind].samples[:, 1], alpha=0.5,\n                  color='orange')\n    # ax[j].scatter(mcmc.intermediate_samples[ind][:, 0], mcmc.intermediate_samples[ind][:, 1], alpha=0.5,\n    #              color='orange')\n    ax[j].set_xlim([-2, 2])\n    ax[j].set_ylim([-2, 2])\n    ax[j].set_title(r'$\\beta$ = {:.3f}'.format(mcmc.tempering_parameters[ind]), fontsize=16)\n    ax[j].set_xlabel(r'$\\theta_{1}$', fontsize=14)\n    ax[j].set_ylabel(r'$\\theta_{2}$', fontsize=14)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_potential(x, temper_param, log_intermediate_values):\n    \"\"\"  \"\"\"\n    return log_intermediate_values / temper_param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ev = mcmc.evaluate_normalization_constant(compute_potential=compute_potential, log_Z0=0.)\nprint('Estimate of evidence by thermodynamic integration = {:.4f}'.format(ev))\n\nev = mcmc.evaluate_normalization_constant(compute_potential=compute_potential, nsamples_from_p0=5000)\nprint('Estimate of evidence by thermodynamic integration = {:.4f}'.format(ev))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reliability\nIn a reliability context, $p_{0}$ is the pdf of the parameters and we have:\n\n\\begin{align}q_{\\beta}(x) = I_{\\beta}(x) = \\frac{1}{1 + \\exp{ \\left( \\frac{G(x)}{1/\\beta-1}\\right)}}\\end{align}\n\nwhere $G(x)$ is the performance function, negative if the system fails, and $I_{\\beta}(x)$ are smoothed\nversions of the indicator function. Then to compute the probability of failure, the potential can be computed as:\n\n\\begin{align}U_{\\beta}(x) = \\frac{- \\frac{G(x)}{(1-\\beta)^2}}{1 + \\exp{ \\left( -\\frac{G(x)}{1/\\beta-1} \\right) }} = - \\frac{1 - I_{\\beta}(x)}{\\beta (1 - \\beta)} \\ln{ \\left[ \\frac{1 - I_{\\beta}(x)}{I_{\\beta}(x)} \\right] }\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n\n\ndef indic_sigmoid(y, beta):\n    return 1. / (1. + np.exp(y / (1. / beta - 1.)))\n\n\nfig, ax = plt.subplots(figsize=(4, 3.5))\nys = np.linspace(-5, 5, 100)\nfor i, s in enumerate(1. / np.array([1.01, 1.25, 2., 4., 70.])):\n    ax.plot(ys, indic_sigmoid(y=ys, beta=s), label=r'$\\beta={:.2f}$'.format(s), color='blue', alpha=1. - i / 6)\nax.set_xlabel(r'$y=g(\\theta)$', fontsize=13)\nax.set_ylabel(r'$q_{\\beta}(\\theta)=I_{\\beta}(y)$', fontsize=13)\n# ax.set_title(r'Smooth versions of the indicator function', fontsize=14)\nax.legend(fontsize=8.5)\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "beta = 2  # Specified Reliability Index\nrho = 0.7  # Specified Correlation\ndim = 2  # Dimension\n\n# Define the correlation matrix\nC = np.ones((dim, dim)) * rho\nnp.fill_diagonal(C, 1)\nprint(C)\n\n# Print information related to the true probability of failure\ne, v = np.linalg.eig(np.asarray(C))\nbeff = np.sqrt(np.max(e)) * beta\nprint(beff)\nfrom scipy.stats import norm\n\npf_true = norm.cdf(-beta)\nprint('True pf={}'.format(pf_true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def estimate_Pf_0(samples, model_values):\n    mask = model_values <= 0\n    return np.sum(mask) / len(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Sample from the prior\nmodel = RunModel(model=PythonModel(model_script='local_reliability_funcs.py', model_object_name=\"correlated_gaussian\",\n                                   b_eff=beff, d=dim))\nsamples = MultivariateNormal(mean=np.zeros((2,)), cov=np.array([[1, 0.7], [0.7, 1]])).rvs(nsamples=20000)\nmodel.run(samples=samples, append_samples=False)\nmodel_values = np.array(model.qoi_list)\n\nprint('Prob. failure (MC) = {}'.format(estimate_Pf_0(samples, model_values)))\n\nfig, ax = plt.subplots(figsize=(4, 3.5))\nmask = model_values <= 0\nax.scatter(samples[np.squeeze(mask), 0], samples[np.squeeze(mask), 1], color='red', label='fail', alpha=0.5, marker='d')\nax.scatter(samples[~np.squeeze(mask), 0], samples[~np.squeeze(mask), 1], color='blue', label='safe', alpha=0.5)\nplt.axis('equal')\nplt.xlabel(r'$\\theta_{1}$', fontsize=13)\nplt.ylabel(r'$\\theta_{2}$', fontsize=13)\nax.legend(fontsize=13)\nfig.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "distribution_reference = MultivariateNormal(mean=np.zeros((2,)), cov=np.array([[1, 0.7], [0.7, 1]]))\n\n\ndef log_factor_temp(x, temper_param):\n    model.run(samples=x, append_samples=False)\n    G_values = np.array(model.qoi_list)\n    return np.squeeze(np.log(indic_sigmoid(G_values, temper_param)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "betas = (1. / np.array([1.01, 1.02, 1.05, 1.1, 1.2, 1.5, 2., 3., 5., 10., 25., 70.]))[::-1]\n\nprint(len(betas))\nprint(betas)\n\nfig, ax = plt.subplots(figsize=(5, 4))\nys = np.linspace(-5, 5, 100)\nfor i, s in enumerate(betas):\n    ax.plot(ys, indic_sigmoid(y=ys, beta=s), label=r'$\\beta={:.2f}$'.format(s), color='blue', alpha=1. - i / 15)\nax.set_xlabel(r'$y=g(\\theta)$', fontsize=13)\nax.set_ylabel(r'$I_{\\beta}(y)$', fontsize=13)\nax.set_title(r'Smooth versions of the indicator function', fontsize=14)\nax.legend()\nplt.show()\n\nscales = [0.1 / np.sqrt(beta) for beta in betas]\nprint(scales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from UQpy.sampling import MetropolisHastings, ParallelTemperingMCMC\n\nseed = -2. + 4. * np.random.rand(5, 2)\n\nprint(betas)\nsamplers = [MetropolisHastings(burn_length=5000, jump=5, seed=list(seed), dimension=2,\n                               proposal_is_symmetric=True,\n                               proposal=JointIndependent([Normal(scale=scale)] * 2)) for scale in scales]\nmcmc = ParallelTemperingMCMC(log_pdf_intermediate=log_factor_temp,\n                             distribution_reference=distribution_reference,\n                             n_iterations_between_sweeps=10,\n                             tempering_parameters=list(betas),\n                             random_state=123,\n                             save_log_pdf=True, samplers=samplers)\n\nmcmc.run(nsamples_per_chain=250)\nprint(mcmc.samples.shape)\nprint(mcmc.mcmc_samplers[0].samples.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=3, figsize=(12, 3.5))\nfor j, ind in enumerate([0, 6, -1]):\n    ax[j].scatter(mcmc.mcmc_samplers[ind].samples[:, 0], mcmc.mcmc_samplers[ind].samples[:, 1], alpha=0.25)\n    ax[j].set_xlim([-4, 4])\n    ax[j].set_ylim([-4, 4])\n    ax[j].set_title(r'$\\beta$ = {:.3f}'.format(mcmc.tempering_parameters[ind]), fontsize=15)\n    ax[j].set_xlabel(r'$\\theta_{1}$', fontsize=13)\n    ax[j].set_ylabel(r'$\\theta_{2}$', fontsize=13)\nfig.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_potential(x, temper_param, log_intermediate_values):\n    indic_beta = np.exp(log_intermediate_values)\n    indic_beta = np.where(indic_beta > 1. - 1e-16, 1. - 1e-16, indic_beta)\n    indic_beta = np.where(indic_beta < 1e-16, 1e-16, indic_beta)\n    tmp_log = np.log((1. - indic_beta) / indic_beta)\n    return - (1. - indic_beta) / (temper_param * (1. - temper_param)) * tmp_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ev = mcmc.evaluate_normalization_constant(compute_potential=compute_potential, log_Z0=np.log(0.5))\nprint('Estimate of evidence by thermodynamic integration = {}'.format(ev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(mcmc.thermodynamic_integration_results['temper_param_list'],\n         mcmc.thermodynamic_integration_results['expect_potentials'], marker='x')\nplt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seed = -2. + 4. * np.random.rand(5, 2)\n\nsamplers = [MetropolisHastings(burn_length=5000, jump=5, seed=list(seed), dimension=2,\n                               proposal_is_symmetric=True,\n                               proposal=JointIndependent([Normal(scale=scale)] * 2)) for scale in scales]\nmcmc = ParallelTemperingMCMC(log_pdf_intermediate=log_factor_temp,\n                             distribution_reference=distribution_reference,\n                             n_iterations_between_sweeps=10,\n                             tempering_parameters=list(betas),\n                             random_state=123,\n                             save_log_pdf=True, samplers=samplers)\n\nlist_ev_0, list_ev_1 = [], []\nnsamples_per_chain = 0\nfor i in range(50):\n    nsamples_per_chain += 50\n    mcmc.run(nsamples_per_chain=nsamples_per_chain)\n    ev = mcmc.evaluate_normalization_constant(compute_potential=compute_potential, log_Z0=np.log(0.5))\n    # print(np.exp(log_ev))\n    list_ev_0.append(ev)\n    ev = mcmc.evaluate_normalization_constant(compute_potential=compute_potential, nsamples_from_p0=100000)\n    list_ev_1.append(ev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 3.5))\nlist_samples = [5 * i * 50 for i in range(1, 51)]\nax.plot(list_samples, list_ev_0)\nax.grid(True)\nax.set_ylabel(r'$Z_{1}$ = proba. failure', fontsize=14)\nax.set_xlabel(r'nb. saved samples per chain', fontsize=14)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
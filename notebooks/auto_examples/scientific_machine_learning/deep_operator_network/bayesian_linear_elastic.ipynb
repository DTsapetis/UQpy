{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Learning a Linear Elastic system\n\nIn this example, we train a Bayesian DeepOperatorNetwork to learn the behavior of a linear elastic system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we use a deep operator network to approximate the solution to a linear elastic system.\nThe dataset is provided by Goswami et al. :cite:`goswami2022elasticity` and our architecture closely follows their design.\nUsing the dataset, we will\n\n1. Construct a deep operator network\n2. Load the training and testing data\n3. Fit the network parameters to the training data\n4. Plot the loss history\n\nFirst, import the necessary modules and set the logging behavior of UQpy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport scipy.io as io\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport UQpy.scientific_machine_learning as sml\n\nlogger = logging.getLogger(\"UQpy\")\nlogger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Construct a Bayesian deep operator network**\n\nA Bayesian Deep Operator Network is defined using the branch network, that encodes information about the domain $x$,\nand the trunk network, that encodes information about the transformation.\n\nThe branch and trunk networks can be defined using a ``torch.nn.Module`` or any subclass of it.\nHere we use subclasses of ``torch.nn.Module`` to define the networks. Both classes use standard\ntorch activation functions and UQpy layers to define their operations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class BranchNet(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.fnn = nn.Sequential(sml.BayesianLinear(101, 100), nn.Tanh())\n        self.conv_layers = nn.Sequential(\n            sml.BayesianConv2d(1, 16, (5, 5), padding=\"same\"),\n            nn.AvgPool2d(2, 1, padding=0),\n            sml.BayesianConv2d(16, 16, (5, 5), padding=\"same\"),\n            nn.AvgPool2d(2, 1, padding=0),\n            sml.BayesianConv2d(16, 16, (5, 5), padding=\"same\"),\n            nn.AvgPool2d(2, 1, padding=0),\n            sml.BayesianConv2d(16, 64, (5, 5), padding=\"same\"),\n            nn.AvgPool2d(2, 1, padding=0),\n        )\n        self.dnn = nn.Sequential(\n            nn.Flatten(),\n            sml.BayesianLinear(64 * 6 * 6, 512),\n            nn.Tanh(),\n            sml.BayesianLinear(512, 512),\n            nn.Tanh(),\n            sml.BayesianLinear(512, 200),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.fnn(x)\n        x = x.view(-1, 1, 10, 10)\n        x = self.conv_layers(x)\n        x = self.dnn(x)\n        return x.unsqueeze(1)\n\n\nclass TrunkNet(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.fnn = nn.Sequential(\n            sml.BayesianLinear(2, 128),\n            nn.Tanh(),\n            sml.BayesianLinear(128, 128),\n            nn.Tanh(),\n            sml.BayesianLinear(128, 128),\n            nn.Tanh(),\n            sml.BayesianLinear(128, 200),\n            nn.Tanh(),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.fnn(x)\n\n\nbranch_network = BranchNet()\ntrunk_network = TrunkNet()\nmodel = sml.DeepOperatorNetwork(branch_network, trunk_network, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Load the training and testing data**\n\nWith the model constructed, we turn our attention to the training data.\nHere we define a subclass of ``torch.nn.Dataset`` as outlined by the [torch documentation](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ElasticityDataSet(Dataset):\n    \"\"\"Load the Elasticity dataset\"\"\"\n\n    def __init__(self, x, f_x, u_x, u_y):\n        self.x = x\n        self.f_x = f_x\n        self.u_x = u_x\n        self.u_y = u_y\n\n    def __len__(self):\n        return int(self.f_x.shape[0])\n\n    def __getitem__(self, i):\n        return self.x, self.f_x[i, :], (self.u_x[i, :, 0], self.u_y[i, :, 0])\n\n\nelastic_data = io.loadmat('linear_elastic_data.mat')\ntrain_dataset, test_dataset = random_split(ElasticityDataSet(\n    elastic_data['X'], elastic_data['F'], elastic_data['Ux'],\n    elastic_data['Uy']), [0.9, 0.1])\n\ntrain_data = DataLoader(train_dataset,\n                        batch_size=20,\n                        shuffle=True,\n                        )\ntest_data = DataLoader(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Fit the network parameters to the training data**\n\nAll that's left is to define a loss function, an optimizer, and run the BBBTrainer.\nWe use the Mean Squared Error loss, KullbackLeibler divergence, and an Adam optimizer from torch.\nWe assemble the model, optimizer, loss function, and training data with UQpy's BBBTrainer to learn the model parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LossFunction(nn.Module):\n    def __init__(self, reduction: str = \"mean\", *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.reduction = reduction\n\n    def forward(self, prediction, label):\n        return F.mse_loss(\n            prediction[:, :, 0], label[0], reduction=self.reduction\n        ) + F.mse_loss(prediction[:, :, 1], label[1], reduction=self.reduction)\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\ntrainer = sml.BBBTrainer(model, optimizer, loss_function=LossFunction(), scheduler=scheduler)\ntrainer.run(\n    train_data=train_data,\n    test_data=test_data,\n    epochs=50,\n    beta=1e-6,\n    num_samples=5,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we plot the training history.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.semilogy(trainer.history[\"train_loss\"], label=\"Train Loss\")\nax.semilogy(trainer.history[\"test_nll\"], label=\"Test NLL\")\nax.set_title(\"Bayesian DeepONet Training History\")\nax.set(xlabel=\"Epoch\", ylabel=\"Loss\")\nax.legend()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Probabilisitc Models with Monte Carlo Dropout\n\nThis example shows how to create a probabilistic neural network using UQpy's ProbabilisticDropout layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we import the necessary modules and, optionally, set UQpy to print logs to the console.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport UQpy.scientific_machine_learning as sml\n\ntorch.manual_seed(0)\n\n# logger = logging.getLogger(\"UQpy\")  # Optional, display UQpy logs to console\n# logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our neural network will approximate the function $f(x)=0.4 \\sin(4x) + 0.5 \\cos(12x) + \\epsilon$ over the domain\n$x \\in [-1, 1]$. $\\epsilon$ represents the noise in our measurement defined as the Gaussian random\nvariable $\\epsilon \\sim N(0, 0.05)$.\n\nBelow we define the dataset by subclassing :py:class:`torch.utils.data.Dataset`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SinusoidalDataset(Dataset):\n    def __init__(self, n_samples=20, noise_std=0.05):\n        self.n_samples = n_samples\n        self.noise_std = noise_std\n        self.x = torch.linspace(-1, 1, n_samples).reshape(-1, 1)\n        self.y =0.4 * torch.sin(4 * self.x) + 0.5 * torch.cos(12 * self.x)\n        self.y += torch.normal(0, self.noise_std, self.x.shape)\n\n    def __len__(self):\n        return self.n_samples\n\n    def __getitem__(self, item):\n        return self.x[item], self.y[item]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define our model as a fully connected feed-forward neural network.\nAfter each hidden layer, we add a :py:class:`ProbabilisticDropout` layer to randomly set some tensor elements to zero\nwith probability $p=1e-3$. These layers will be *inactive*  during training,\nand will only be turned on during evaluation.\n\nThe model is trained using Pytorch's gradient descent algorithms passed to :py:class:`Trainer`.\nWe include a scheduler to control the learning rate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "width = 30\np = 2e-3\nnetwork = nn.Sequential(\n    nn.Linear(1, width),\n    nn.ReLU(),\n    nn.Linear(width, width),\n    sml.ProbabilisticDropout(p=p),\n    nn.ReLU(),\n    nn.Linear(width, width),\n    sml.ProbabilisticDropout(p=p),\n    nn.ReLU(),\n    nn.Linear(width, 1),\n)\nmodel = sml.FeedForwardNeuralNetwork(network)\nmodel.drop(False)  # turn off all ProbabilisticDropout layers\n\ndataset = SinusoidalDataset()\ntrain_data = DataLoader(dataset, batch_size=20, shuffle=False)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1_000)\ntrainer = sml.Trainer(model, optimizer, scheduler=scheduler)\nprint(\"Starting Training...\", end=\"\")\ntrainer.run(train_data=train_data, epochs=5_000)\nprint(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's the hard part done! We defined our dataset, our model, and then fit the model to the data.\nThe rest of this example plots the model predictions to compare them to the exact solution.\nWe also show how to activate the dropout layers to compute a probabilistic prediction\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot training history\nfig, ax = plt.subplots()\nax.semilogy(trainer.history[\"train_loss\"])\nax.set_title(\"Training Loss\")\nax.set(xlabel=\"Epoch\", ylabel=\"Loss\")\nfig.tight_layout()\n\n# Plot model predictions\nx_noisy = dataset.x\ny_noisy = dataset.y\nx_exact = torch.linspace(-1, 1, 1000).reshape(-1, 1)\ny_exact = 0.4 * torch.sin(4 * x_exact) + 0.5 * torch.cos(12 * x_exact)\n\nmodel.eval()\n# compute deterministic prediction\nmodel.drop(False)\nprint(\"Model is dropping:\", model.dropping)\nwith torch.no_grad():\n    deterministic_prediction = model(x_exact)\n\n# compute probabilistic predictions\nmodel.drop()  # turn on all dropout layers\nprint(\"Model is dropping:\", model.dropping)\nn = 500\nsamples = torch.zeros(n, len(x_exact))\nfor i in range(n):\n    samples[i, :] = model(x_exact).detach().squeeze()\nquantile_low = torch.quantile(samples, q=0.025, dim=0)\nquantile_high = torch.quantile(samples, q=0.975, dim=0)\n\nfig, ax = plt.subplots()\nax.scatter(x_noisy, y_noisy, label=\"Training Data\", color=\"black\")\nax.plot(\n    x_exact,\n    y_exact,\n    label=\"Exact\",\n    color=\"black\",\n    linestyle=\"dashed\",\n)\nax.plot(\n    x_exact,\n    deterministic_prediction,\n    label=\"Deterministic Model\",\n    color=\"tab:blue\",\n)\nax.fill_between(\n    x_exact.squeeze(),\n    quantile_low,\n    quantile_high,\n    label=\"Middle 95%\",\n    color=\"tab:blue\",\n    alpha=0.3,\n)\nax.set_title(\"Monte Carlo Dropout Predictions\")\nax.set(xlabel=\"x\", ylabel=\"f(x)\")\nax.legend()\n\n\n# Plotting Results\n# x_data = SinusoidalDataset().x.detach()\n# y_data = SinusoidalDataset().y.detach()\n# x_val = torch.linspace(-1, 1, 1000).view(-1, 1).detach()\n# y_val = 0.4 * torch.sin(4 * x_val) + 0.5 * torch.cos(12 * x_val).detach()\n# pred_val = model(x_val).detach()\n#\n# # %% Plot the deterministic model estimates\n# fig, ax = plt.subplots()\n# ax.scatter(x_data, y_data, label=\"Data\", color=\"black\", s=50)\n# ax.plot(\n#     x_val,\n#     pred_val,\n#     label=\"Final Prediction\",\n#     color=\"tab:orange\",\n# )\n# ax.plot(\n#     x_val.detach(),\n#     y_val.detach(),\n#     label=\"Target\",\n#     color=\"black\",\n#     linestyle=\"dashed\",\n# )\n# ax.set_title(\"Deterministic Prediction\")\n# ax.set(xlabel=\"$x$\", ylabel=\"$f(x)$\")\n# ax.legend()\n# fig.tight_layout()\n#\n# train_loss = trainer.history[\"train_loss\"].detach().numpy()\n# fig, ax = plt.subplots()\n# ax.semilogy(train_loss)\n# ax.set_title(\"Training Loss\")\n# ax.set(xlabel=\"Epoch\", ylabel=\"Loss\")\n# fig.tight_layout()\n#\n# # %%\n# model.drop()  # activate the dropout layers\n# n = 1_000\n# samples = torch.zeros(n, len(x_val))\n# for i in range(n):\n#     samples[i, :] = model(x_val).detach().squeeze()\n# mean = torch.mean(samples, dim=1)\n# standard_deviation = torch.std(samples, dim=1)\n#\n# # Plotting Results\n# fig, ax = plt.subplots()\n# ax.plot(x_val, samples[:, 1], \"tab:orange\", label=\"Prediction 1\")\n# ax.plot(x_val, samples[:, 2], \"tab:blue\", label=\"Prediction 2\")\n# ax.scatter(x_data, y_data, color=\"black\", label=\"Data\")\n# ax.plot(x_val, y_val, color=\"black\", linestyle=\"dashed\", label=\"Target\")\n# ax.set_title(\"Two Samples from Dropout NN\")\n# ax.set(xlabel=\"$x$\", ylabel=\"$f(x)$\")\n# ax.legend()\n# fig.tight_layout()\n#\n# # %%\n# fig, ax = plt.subplots()\n# ax.plot(x_val, mean, label=\"$\\mu$\")\n# ax.fill_between(\n#     x_val.view(-1),\n#     torch.quantile(samples, q=0.025, dim=1),\n#     torch.quantile(samples, q=0.975, dim=1),\n#     label=\"95% Range\",\n#     # mean - (3 * standard_deviation),\n#     # mean + (3 * standard_deviation),\n#     # label=\"$\\mu \\pm 3\\sigma$,\",\n#     alpha=0.3,\n# )\n# ax.plot(x_val, y_val, label=\"Target\", color=\"black\")\n# ax.set_title(\"Dropout Neural Network 95% Range\")\n# ax.set(xlabel=\"x\", ylabel=\"f(x)\")\n# ax.legend()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Bayesian Quickstart Testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the second half of a Bayesian version of the classification problem from this Pytorch Quickstart tutorial:\nhttps://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n\nThis script assumes you have already run the Bayesian Quickstart Testing script and saved the optimized model\nto a file named ``bayesian_model.pth``. This script\n\n- Loads a trained model from the file ``bayesian_model.pt``\n- Makes deterministic predictions\n- Makes probabilistic predictions\n- Plots probabilistic predictions\n\nFirst, we import the necessary modules and define the BayesianNeuralNetwork class, so we can load the model state\ndictionary saved in ``bayesian_model.pth``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\nimport UQpy.scientific_machine_learning as sml\n\nplt.style.use(\"ggplot\")\n\n\nclass BayesianNeuralNetwork(nn.Module):\n    \"\"\"UQpy: Replace torch's nn.Linear with UQpy's sml.BayesianLinear\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            sml.BayesianLinear(28 * 28, 512),  # nn.Linear(28 * 28, 512)\n            nn.ReLU(),\n            sml.BayesianLinear(512, 512),  # nn.Linear(512, 512)\n            nn.ReLU(),\n            sml.BayesianLinear(512, 512),  # nn.Linear(512, 512)\n            nn.ReLU(),\n            sml.BayesianLinear(512, 10),  # nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the model is already trained, we only need to load test data and can ignore the training data.\nThen we use Pytorch's framework for loading a model from a state dictionary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=False,\n    transform=ToTensor(),\n)\n\ndevice = \"cpu\"\nnetwork = BayesianNeuralNetwork().to(device)\nmodel = sml.FeedForwardNeuralNetwork(network).to(device)\nmodel.load_state_dict(torch.load(\"bayesian_model.pt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we make the deterministic prediction. We set the sample mode to ``False`` and evaluate our model on the first\nimage in ``test_data``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\nmodel.eval()\nmodel.sample(False)  # UQpy: Set sample mode to False\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    x = x.to(device)\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(\"----- Deterministic Prediction\")\n    print(f\"Predicted: {predicted}, Actual: {actual}\")\n    print(f\"{'Class'.ljust(11)} {'Logits'.rjust(7)} {'softmax(Logits)'}\")\n    for i, c in enumerate(classes):\n        print(f\"{c.ljust(12)} {pred[0, i]: 6.2f} {torch.softmax(pred, 1)[0, i]: 15.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just like the torch tutorial, our model correctly identifies this image as an ankle boot.\nNext, we show how to make probabilistic predictions by turning on our model's ``sampling`` mode.\nWe feed the same image of an ankle boot into our model 1,000 times and each time the weights and biases\nare sampled from the distributions that were learned during training.\nRather than one static output, we get a distribution of 1,000 predictions!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.sample()\nn_samples = 1_000\nlogits = torch.empty((n_samples, len(classes)))\nwith torch.no_grad():\n    for i in range(n_samples):\n        logits[i] = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those few lines are all it takes to make Bayesian predictions.\nThe rest of this example converts the predicted logits into class predictions.\nThe predicted distribution is visualized in the histogram below, which\nshows the majority of predictions classify the image as an ankle boot.\nInterestingly, the two other shoe classes (sneaker and sandal) are also common predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_classes = torch.argmax(logits, dim=1)\npredicted_counts = torch.bincount(predicted_classes)\npredictions = {c: int(i) for c, i in zip(classes, predicted_counts)}\ni = torch.argmax(predicted_counts)\n\nprint(\"----- Probabilistic Predictions\")\nprint(\"Most Commonly Predicted:\", classes[i], \"Actual:\", actual)\nprint(f\"{'Class'.ljust(11)} Probability\")\nfor c in classes:\n    print(f\"{c.ljust(11)} {predictions[c] / n_samples: 11.3f}\")\n\n# Plot probabilistic class predictions\nfig, ax = plt.subplots()\ncolors = plt.cm.tab10_r(torch.linspace(0, 1, 10))\nb = ax.bar(classes, predicted_counts, label=predicted_counts, color=colors)\nax.bar_label(b)\nax.set_title(\"Bayesian NN Predictions\", loc=\"left\")\nax.set(xlabel=\"Classes\", ylabel=\"Counts\")\nax.set_xticklabels(classes, rotation=45)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition to visualizing the class predictions, we can look at the logit values output by our Bayesian model.\nBelow we plot a histogram of the logits for Sandal, Sneaker, Bag, and Ankle Boot.\nThe other classes are omitted since they were never predicted by our model.\n\nNotice that for Bag, the histogram peaks near zero.\nThis aligns with the fact that Bag is a very unlikely prediction from our model.\nContrast that histogram with the one for Ankle Boot, which is very likely to take on a high value.\nWe interpret this as our model being very likely to predict Ankle Boot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot probabilistic logit predictions\nsoftmax_logits = torch.softmax(logits, 1)\nfig, ax = plt.subplots()\nax.hist(softmax_logits[:, 9], label=\"Ankle Boot\", bins=20, facecolor=colors[9])\nfor i in (5, 7, 8):  # loop over Sandal, Sneaker, Bag\n    ax.hist(\n        softmax_logits[:, i],\n        label=classes[i],\n        bins=20,\n        edgecolor=colors[i],\n        facecolor=\"none\",\n        linewidth=2,\n    )\nax.set_title(\"Bayesian NN softmax(logits)\", loc=\"left\")\nax.set(xlabel=\"softmax(logit) Value\", ylabel=\"Log(Counts)\")\nax.legend()\nax.set_yscale(\"log\")\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also visualize these distributions and how they correlate with one another with a pair plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Use pandas and seaborn for easy pair plots\nimport seaborn\nimport pandas as pd\n\ndf = pd.DataFrame({classes[i]: softmax_logits[:, i] for i in (5, 7, 8, 9)})\nseaborn.pairplot(df, corner=True, plot_kws={\"alpha\": 0.2, \"edgecolor\": None})\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
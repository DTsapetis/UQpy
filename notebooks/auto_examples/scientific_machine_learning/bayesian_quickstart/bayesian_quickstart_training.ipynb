{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Bayesian Quickstart Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the first half of a Bayesian version of the classification problem from this Pytorch Quickstart tutorial:\n(https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n\nWe strongly recommend reading the Pytorch quick start first to familiarize yourself with the problem.\nWe include many of the comments from the Pytorch example, but assume the reader is familiar with model definitions\nand parameter optimization in Pytorch.\nIn their tutorial, Pytorch implements a fully connected deterministic neural network to learn a classification of\narticles of clothing. Here, we implement a fully connected *Bayesian* neural network to learn the same classification.\n\nWe import all the same packages, with the addition of UQpy's scientific machine learning module.\nNote that this demo requires the ``torchvision`` package.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport UQpy.scientific_machine_learning as sml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The FashionMNIST dataset :cite:`xiao2017fashionMNIST` and dataloaders for our Bayesian classifier are identical to those used in the\ndeterministic case.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\nbatch_size = 64\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To construct a Bayesian classifier, we use UQpy's ``BayesianLinear`` layers in the ``nn.Module`` subclass.\nThe ``BayesianLinear`` takes in the in features and out features just like ``nn.Linear``.\nThe key difference is while the standard Linear layer has deterministic numbers for every element in its weight\nand bias tensors, a BayesianLinear layer has a representation of a random variable for each element in its tensors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get cpu, gpu or mps device for training.\ndevice = \"cpu\"\n# device = (\n#     \"cuda\"\n#     if torch.cuda.is_available()\n#     else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n# )\nprint(f\"Using {device} device\")\n\n\n# Define model\nclass BayesianNeuralNetwork(nn.Module):\n    \"\"\"UQpy: Replace torch's nn.Linear with UQpy's sml.BayesianLinear\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            sml.BayesianLinear(28 * 28, 512),  # nn.Linear(28 * 28, 512)\n            nn.ReLU(),\n            sml.BayesianLinear(512, 512),  # nn.Linear(512, 512)\n            nn.ReLU(),\n            sml.BayesianLinear(512, 512),  # nn.Linear(512, 512)\n            nn.ReLU(),\n            sml.BayesianLinear(512, 10),  # nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just like in Torch tutorials, we define our network using an instance of this custom class.\nThe network is wrapped inside the ``sml.FeedForwardNeuralNetwork`` object, which does not change the network but gives\nus better control over the random variable behavior. By default, the model is set to sampling mode, where the weights\nand biases of each Bayesian layer are sampled from their governing distribution. We can turn sampling off,\nwhich causes the model to use the means of its distributions (and consequently behave deterministically) with the\ncommand ``model.sample(False)``. To turn sampling back on, use ``model.sample(True)``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "network = BayesianNeuralNetwork().to(device)\nmodel = sml.FeedForwardNeuralNetwork(network).to(device)\nprint(model)\nmodel.sample(False)\nprint(\"model is in sampling mode:\", model.sampling)\nmodel.sample()\nprint(\"model is in sampling mode:\", model.sampling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With our model defined, we can turn our attention to training. Training a Bayesian neural network uses Torch's\n``optim`` library, and we can reuse almost all of their training and testing functions.\nWe use the testing function from torch's quickstart tutorial, with a small modification to the loss function.\nWe add a divergence term to the loss, which represents the likelihood of the posterior distribution in the Bayesian\nupdate. The default prior distribution in a Bayesian layer is a zero mean Gaussian, so this effectively acts as a\nregularization term driving the weights and biases towards zero. We scale down the divergence by a factor of\n$10^{-6}$ so it doesn't dominate the total loss.\n\nThe test function is nearly identical to torch's, with the inclusion of a single line to ensure the\nsampling mode is set to ``False``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n# UQpy: define divergence function used in loss\ndivergence_fn = sml.GaussianKullbackLeiblerDivergence()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n\ndef train(dataloader, model, loss_fn, optimizer):\n    \"\"\"UQpy: Include a divergence term in the loss\"\"\"\n    size = len(dataloader.dataset)\n    model.train()\n    # UQpy: Set to sample mode to True.\n    model.sample()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # UQpy: Compute divergence and add it to the data loss\n        beta = 1e-6\n        loss += beta * divergence_fn(model)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test(dataloader, model, loss_fn):\n    \"\"\"UQpy: ensure model sampling is turned off.\"\"\"\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    # UQpy: Set sampling mode to False\n    model.sample(False)\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(\n        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n    )\n\n\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n\ntorch.save(model.state_dict(), \"bayesian_model.pt\")\nprint(\"Saved PyTorch Model State to bayesian_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's it! We defined the Bayesian neural network using UQpy's layers and added the model divergence to the loss\nfunction. That's the basics of defining and training a Bayesian neural network.\n\nIn the next tutorial we make predictions with this trained Bayesian model.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gaussian Process with Noise and Constraints\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This jupyter script shows the performance of GaussianProcessRegressor class in the UQpy. A training data is generated\nusing a function ($f(x)$, as defined below), which is used to train a surrogate model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the necessary modules to run the example script. Notice that FminCobyla is used here, to solve the MLE\noptimization problem with constraints.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nfrom UQpy.surrogates.gaussian_process.regression_models.QuadraticRegression import QuadraticRegression\n\nwarnings.filterwarnings('ignore')\nfrom UQpy.utilities.MinimizeOptimizer import MinimizeOptimizer\nfrom UQpy.utilities.FminCobyla import FminCobyla\nfrom UQpy.surrogates import GaussianProcessRegression, NonNegative, RBF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the following function $f(x)$.\n\n\\begin{align}f(x) = \\frac{1}{100} + \\frac{5}{8}(2x-1)^4[(2x-1)^2 + 4\\sin{(5 \\pi x)^2}], \\quad \\quad x \\in [0,1]\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def funct(x):\n    y = (1 / 100) + (5 / 8) * ((2 * x - 1) ** 4) * (((2 * x - 1) ** 2) + 4 * np.sin(5 * np.pi * x) ** 2)\n    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the training data set. The following 13 points have been used to fit the GP.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train = np.array([0, 0.06, 0.08, 0.26, 0.27, 0.4, 0.52, 0.6, 0.68, 0.81, 0.9, 0.925, 1]).reshape(-1, 1)\ny_train = funct(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the test data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_test = np.linspace(0, 1, 100).reshape(-1, 1)\ny_test = funct(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train GPR\n- Noise\n- Constraints\n\nHere, 30 equidistant point are selected over the domain of $x$, lets call them constraint points. The idea is to\ntrain the surrogate model such that the probability of positive surrogates prediction is very high at these points.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_c = np.linspace(0, 1, 31).reshape(-1,1)\ny_c = funct(X_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this approach, MLE problem is solved with the following constraints:\n\n\\begin{align}\\hat{y}(x_c)-Z \\sigma_{\\hat{y}}(x_c) > 0  \\quad \\quad Z = 2\\end{align}\n\\begin{align}|\\hat{y}(x_t) - y(x_t)| < \\epsilon   \\quad \\quad \\epsilon = 0.3\\end{align}\n\nwhere, $x_c$ and $x_t$ are the constraint and training sample points, respectively.\n\nDefine kernel used to define the covariance matrix. Here, the application of Radial Basis Function (RBF) kernel is\ndemonstrated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel3 = RBF()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the optimizer used to identify the maximum likelihood estimate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bounds_3 = [[10**(-6), 10**(-1)], [10**(-5), 10**(-1)], [10**(-13), 10**(-5)]]\noptimizer3 = FminCobyla()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define constraints for the Cobyla optimizer using UQpy's Nonnegatice class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cons = NonNegative(constraint_points=X_c, observed_error=0.03, z_value=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the 'GaussianProcessRegressor' class object, the input attributes defined here are kernel, optimizer, initial\nestimates of hyperparameters and number of times MLE is identified using random starting point.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gpr3 = GaussianProcessRegression(kernel=kernel3, hyperparameters=[10**(-3), 10**(-2), 10**(-10)], optimizer=optimizer3,\n                                 optimizations_number=10, optimize_constraints=cons, bounds=bounds_3, noise=True,\n                                 regression_model=QuadraticRegression())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Call the 'fit' method to train the surrogate model (GPR).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gpr3.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The maximum likelihood estimates of the hyperparameters are as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(gpr3.hyperparameters)\n\nprint('Length Scale: ', gpr3.hyperparameters[0])\nprint('Process Variance: ', gpr3.hyperparameters[1])\nprint('Noise Variance: ', gpr3.hyperparameters[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use 'predict' method to compute surrogate prediction at the test samples. The attribute 'return_std' is a boolean\nindicator. If 'True', 'predict' method also returns the standard error at the test samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_pred3, y_std3 = gpr3.predict(X_test, return_std=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot shows the test function in dashed red line and 13 training points are represented by blue dots. Also, blue\ncurve shows the GPR prediction for $x \\in (0, 1)$ and yellow shaded region represents 95% confidence interval.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8.5,7))\nax.plot(X_test,y_test,'r--',linewidth=2,label='Test Function')\nax.plot(X_train,y_train,'bo',markerfacecolor='b', markersize=10, label='Training Data')\nax.plot(X_test,y_pred3,'b-', lw=2, label='GP Prediction')\nax.plot(X_test, np.zeros((X_test.shape[0],1)))\nax.fill_between(X_test.flatten(), y_pred3-1.96*y_std3,\n                y_pred3+1.96*y_std3,\n                facecolor='yellow',label='95% Credibility Interval')\nax.tick_params(axis='both', which='major', labelsize=12)\nax.set_xlabel('x', fontsize=15)\nax.set_ylabel('f(x)', fontsize=15)\nax.set_ylim([-0.3,1.8])\nax.legend(loc=\"upper right\",prop={'size': 12});\nplt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the constraints for the trained surrogate model. Notice that all values are positive, thus constraints are\nsatisfied for the constraint points.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_, ys_ = gpr3.predict(X_c, return_std=True)\ny_ - 2*ys_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that all values are negative, thus constraints are satisfied for the training points.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_ = gpr3.predict(X_train, return_std=False)\nnp.abs(y_train[:, 0]-y_) - 0.03"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Diagnostics for MCMC methods\n\nThis notebook illustrates the use of a few diagnostics for :class:`.MCMC`. The example consists in learning the\nparameters of a regression model via Bayesian inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the necessary libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from UQpy.sampling import MetropolisHastings\nfrom UQpy.distributions import Normal, JointIndependent\nimport numpy as np\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example setup\nFirst generate data then define the target log pdf = log likelihood $p(data \\vert x)$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "param_true = [1., 2.]\ndomain = np.linspace(0, 10, 50)\ndata_clean = param_true[0] * domain + param_true[1] * domain ** 2\nrstate = np.random.RandomState(123)\ndata_noisy = data_clean + rstate.randn(*data_clean.shape)\n\nfrom scipy.stats import norm\n\n\ndef log_target(x, data, x_domain):\n    log_target_value = np.zeros(x.shape[0])\n    for i, xx in enumerate(x):\n        h_xx = xx[0] * x_domain + xx[1] * x_domain ** 2\n        log_target_value[i] = np.sum([norm.logpdf(hxi - datai) for hxi, datai in zip(h_xx, data)])\n    return log_target_value\n\n\nproposal = JointIndependent([Normal(scale=0.1), Normal(scale=0.05)])\n\nsampler = MetropolisHastings(nsamples=500, dimension=2, log_pdf_target=log_target,\n                             burn_length=10, jump=10, n_chains=1,\n                             args_target=(data_noisy, domain), proposal=proposal)\n\nprint(sampler.samples.shape)\nsamples = sampler.samples\n\nplt.plot(samples[:, 0], samples[:, 1], 'o', alpha=0.5)\nplt.plot(1., 2., marker='x', color='orange')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Look at acceptance rate of the chain(s)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(sampler.acceptance_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This acceptance rate is quite low, showing that the proposal should probably be refined !\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphics\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15, 7))\nfor j in range(samples.shape[1]):\n    ax[j, 0].plot(np.arange(samples.shape[0]), samples[:, j])\n    ax[j, 0].set_title('chain - parameter # {}'.format(j + 1))\n    ax[j, 1].plot(np.arange(samples.shape[0]), np.cumsum(samples[:, j]) / np.arange(samples.shape[0]))\n    ax[j, 1].set_title('parameter convergence')\n    ax[j, 2].acorr(samples[:, j] - np.mean(samples[:, j]), maxlags=40, normed=True)\n    ax[j, 2].set_title('correlation between samples')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute effective sample size (ESS)\nIn MCMC, the $ESS$ has been used to derive termination rules, based on the quality of the estimation. In brief,\nthe simulation stops when the computational uncertainty on a chosen quantity (for instance, the expected value of RV\n$x$) is small compared to its posterior uncertainty [1]. Mathematically, this allows computation of the\n$ESS$ and a minimum value $ESS_{min}$ (qualitatively, an acceptable approximation is reached if\n$ESS > ESS_{min}$). These quantities can be computed by looking at each marginal density (is $x$ is a\nmultivariate random variable), or by looking at the joint. The reader is referred to [1, 2] for more details.\n\n[1] *A practical sequential stopping rule for high-dimensional MCMC and its application to spatial-temporal Bayesian\nmodels*, Gong and Flegal, 2014.\n\n[2] *Multivariate Output Analysis for Markov Chain Monte Carlo*, Vats and Flegal, 2015\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_marginal_ESS(samples, eps_ESS=0.05, alpha_ESS=0.05):\n    # Computation of ESS and min ESS, look at each marginal distribution separately\n    nsamples, dim = samples.shape\n\n    bn = np.ceil(nsamples ** (1 / 2))  # nb of samples per bin\n    an = int(np.ceil(nsamples / bn))  # nb of bins\n    idx = np.array_split(np.arange(nsamples), an)\n\n    means_subdivisions = np.empty((an, samples.shape[1]))\n    for i, idx_i in enumerate(idx):\n        x_sub = samples[idx_i, :]\n        means_subdivisions[i, :] = np.mean(x_sub, axis=0)\n    Omega = np.cov(samples.T)\n    Sigma = np.cov(means_subdivisions.T)\n\n    marginal_ESS = np.empty((dim,))\n    min_marginal_ESS = np.empty((dim,))\n    for j in range(dim):\n        marginal_ESS[j] = nsamples * Omega[j, j] / Sigma[j, j]\n        min_marginal_ESS[j] = 4 * norm.ppf(alpha_ESS / 2) ** 2 / eps_ESS ** 2\n\n    return marginal_ESS, min_marginal_ESS\n\n\ndef compute_joint_ESS(samples, eps_ESS=0.05, alpha_ESS=0.05):\n    # Computation of ESS and min ESS, look at the joint distribution separately\n    nsamples, dim = samples.shape\n\n    bn = np.ceil(nsamples ** (1 / 2))  # nb of samples per bin\n    an = int(np.ceil(nsamples / bn))  # nb of bins\n    idx = np.array_split(np.arange(nsamples), an)\n\n    means_subdivisions = np.empty((an, samples.shape[1]))\n    for i, idx_i in enumerate(idx):\n        x_sub = samples[idx_i, :]\n        means_subdivisions[i, :] = np.mean(x_sub, axis=0)\n    Omega = np.cov(samples.T)\n    Sigma = np.cov(means_subdivisions.T)\n\n    from scipy.stats import chi2\n    from scipy.special import gamma\n    joint_ESS = nsamples * np.linalg.det(Omega) ** (1 / dim) / np.linalg.det(Sigma) ** (1 / dim)\n    chi2_value = chi2.ppf(1 - alpha_ESS, df=dim)\n    min_joint_ESS = 2 ** (2 / dim) * np.pi / (dim * gamma(dim / 2)) ** (\n            2 / dim) * chi2_value / eps_ESS ** 2\n\n    return joint_ESS, min_joint_ESS\n\n\nmarginal_ESS, min_marginal_ESS = compute_marginal_ESS(samples, eps_ESS=0.05, alpha_ESS=0.05)\n\nprint('Univariate Effective Sample Size in each dimension:')\nfor j in range(2):\n    print('Dimension {}: ESS = {}, minimum ESS recommended = {}'.\n          format(j + 1, marginal_ESS[j], min_marginal_ESS[j]))\n\n    joint_ESS, min_joint_ESS = compute_joint_ESS(samples, eps_ESS=0.05, alpha_ESS=0.05)\n\n    print('\\nMultivariate Effective Sample Size:')\n    print('Multivariate ESS = {}, minimum ESS recommended = {}'.format(joint_ESS, min_joint_ESS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results above show that the chains should probably be run for a longer amount of time in order to obtain reliable\nresults in terms of the expected value of the parameters to be learnt.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
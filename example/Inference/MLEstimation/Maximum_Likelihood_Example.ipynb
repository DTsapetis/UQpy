{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation with UQpy\n",
    "\n",
    "Authors: Audrey Olivier, Dimitris G. Giovanis <br>\n",
    "Last modified on 05/13/2020 by Audrey Olivier\n",
    "\n",
    "This notebook illustrates the use of the Model class to create a model for inference, and the MLEstimation class to perform maximum likelihood estimation of the parameters of that model. Recall that a maximum likelihood estimate is simply the parameter vector that maximizes the likelihood:\n",
    "\n",
    "$$ \\theta_{ML} = argmax_{\\Theta} \\quad p(data \\vert \\theta) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from UQpy.inference import DistributionModel,ComputationalModel, MLE\n",
    "from UQpy.distributions import Normal\n",
    "from UQpy.inference import MinimizeOptimizer\n",
    "from UQpy.distributions import JointIndependent, JointCopula, Gumbel\n",
    "from UQpy.sampling import ImportanceSampling, ISInput\n",
    "from UQpy.RunModel import RunModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a simple probability distribution model\n",
    "\n",
    "In the following we learn the mean and covariance of a univariate gaussian distribution from data.\n",
    "\n",
    "First, for the sake of this example, we generate fake data from a gaussian distribution with mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1  # true mean and standard deviation\n",
    "data_1 = np.random.normal(mu, sigma, 1000).reshape((-1, 1))\n",
    "print('Shape of data vector: {}'.format(data_1.shape))\n",
    "\n",
    "count, bins, ignored = plt.hist(data_1, 30, density=True)\n",
    "plt.plot(bins, 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(- (bins - mu) ** 2 / (2 * sigma ** 2)),\n",
    "         linewidth=2, color='r')\n",
    "plt.title('Histogram of the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the class Model. The user must define the number of parameters to be estimated, in this case 2 (mean and standard deviation), and set those parameters to be learnt as None when instantiating the Distribution object. For maximum likelihood estimation, no prior pdf is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters to be learnt as None\n",
    "dist = Normal(loc=None, scale=None)\n",
    "candidate_model = DistributionModel(parameters_number=2, distributions=dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ml_estimator = MLE(inference_model=candidate_model, data=data_1, optimizations_number=3)\n",
    "print('ML estimates of the mean={0:.3f} (true=0.) and std. dev={1:.3f} (true=0.1)'.format(\n",
    "    ml_estimator.mle[0], ml_estimator.mle[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fix one of the parameters and learn the remaining one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Normal(loc=0., scale=None)\n",
    "candidate_model = DistributionModel(parameters_number=1, distributions=d)\n",
    "\n",
    "optimizer = MinimizeOptimizer(bounds=[[0.0001, 2.]])\n",
    "ml_estimator = MLE(inference_model=candidate_model, data=data_1,\n",
    "                   optimizations_number=1)\n",
    "print('ML estimates of the std. dev={0:.3f} (true=0.1)'.format(ml_estimator.mle[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn a more complex probability model\n",
    "\n",
    "Here we define a bivariate probability model, with a dependence structure defined using a gumbel copula. The goal of inference is to learn the paremeters of the Gaussian marginals and the copula parameter, i.e., the model has 5 unknown parameters. \n",
    "\n",
    "First data is generated from a true model. A distribution with copulas does not possess a fit method, thus sampling is performed using importance sampling/resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_true exhibits dependence between the two dimensions, defined using a gumbel copula\n",
    "dist_true = JointCopula(marginals=[Normal(), Normal()], copula=Gumbel(theta=2.))\n",
    "\n",
    "# generate data using importance sampling: sample from a bivariate gaussian without copula, then weight samples\n",
    "is_input = ISInput()\n",
    "is_input.proposal = JointIndependent(marginals=[Normal(), Normal()])\n",
    "is_input.log_pdf_target = dist_true.log_pdf\n",
    "u = ImportanceSampling(is_input=is_input, samples_number=500)\n",
    "print(u.samples.shape)\n",
    "print(u.weights.shape)\n",
    "# Resample to obtain 5,000 data points\n",
    "u.resample(samples_number=5000)\n",
    "data_2 = u.unweighted_samples\n",
    "print('Shape of data: {}'.format(data_2.shape))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data_2[:, 0], data_2[:, 1], alpha=0.2)\n",
    "ax.set_title('Data points from true bivariate normal with gumbel dependency structure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a model for inference, the user must create a custom file, here bivariate_normal_gumbel.py, to compute the log_pdf of the distribution, given a bivariate data matrix and a parameter vector of length 5. Note that for any probability model that is not one of the simple univariate pdfs supported by UQpy, such a custom file will be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_guess = JointCopula(marginals=[Normal(loc=None, scale=None), Normal(loc=None, scale=None)],\n",
    "                      copula=Gumbel(theta=None))\n",
    "print(d_guess.get_parameters())\n",
    "candidate_model = DistributionModel(parameters_number=5, distributions=d_guess)\n",
    "print(candidate_model.list_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling MLEstimation, the function minimize from the scipy.optimize package is used by default. The user can define bounds for the optimization, a seed, the algorithm to be used, and set the algorithm to perform several optimization iterations, starting at a different random seed every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = MinimizeOptimizer(bounds=[[-5, 5], [0, 10], [-5, 5], [0, 10], [1.1, 4]],\n",
    "                              method=\"SLSQP\")\n",
    "ml_estimator = MLE(inference_model=candidate_model, data=data_2, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_estimator.run(initial_guess=[1., 1., 1., 1., 4.])\n",
    "\n",
    "print('ML estimates of the mean={0:.3f} and std. dev={1:.3f} of 1st marginal (true: 0.0, 1.0)'.\n",
    "      format(ml_estimator.mle[0], ml_estimator.mle[1]))\n",
    "print('ML estimates of the mean={0:.3f} and std. dev={1:.3f} of 2nd marginal (true: 0.0, 1.0)'.\n",
    "      format(ml_estimator.mle[2], ml_estimator.mle[3]))\n",
    "print('ML estimates of the copula parameter={0:.3f} (true: 2.0)'.format(ml_estimator.mle[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, some known parameters can be fixed during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_guess = JointCopula(marginals=[Normal(loc=None, scale=None), Normal(loc=0., scale=1.)],\n",
    "                      copula=Gumbel(theta=None))\n",
    "candidate_model = DistributionModel(parameters_number=3, distributions=d_guess)\n",
    "\n",
    "optimizer = MinimizeOptimizer(bounds=[[-5, 5], [0, 10], [1.1, 4]],\n",
    "                              method=\"SLSQP\")\n",
    "ml_estimator = MLE(inference_model=candidate_model, data=data_2, optimizer=optimizer)\n",
    "ml_estimator.run(initial_guess=[1., 1., 4.])\n",
    "\n",
    "print('ML estimates of the mean={0:.3f} and std. dev={1:.3f} of 1st marginal (true: 0.0, 1.0)'.\n",
    "      format(ml_estimator.mle[0], ml_estimator.mle[1]))\n",
    "print('ML estimates of the copula parameter={0:.3f} (true: 2.0)'.format(ml_estimator.mle[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model\n",
    "\n",
    "Here a model is defined that is of the form \n",
    "\n",
    "$$y=f(\\theta) + \\epsilon$$\n",
    "\n",
    "where f consists in running RunModel. In particular, here $f(\\theta)=\\theta_{0} x + \\theta_{1} x^{2}$ is a regression model.\n",
    "\n",
    "First we generate synthetic data, and add some noise to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "\n",
    "param_true = np.array([1.0, 2.0]).reshape((1, -1))\n",
    "print('Shape of true parameter vector: {}'.format(param_true.shape))\n",
    "\n",
    "# h_func = RunModel(model_script='pfn_models.py', model_object_name='model_quadratic_vectorized',\n",
    "#                  var_names=['theta_0', 'theta_1'])\n",
    "h_func = RunModel(model_script='pfn_models.py', model_object_name='model_quadratic', vec=False,\n",
    "                  var_names=['theta_0', 'theta_1'])\n",
    "h_func.run(samples=param_true)\n",
    "\n",
    "# Add noise\n",
    "error_covariance = 1.\n",
    "data_clean = np.array(h_func.qoi_list[0])\n",
    "noise = Normal(loc=0., scale=np.sqrt(error_covariance)).rvs(nsamples=50).reshape((50,))\n",
    "data_3 = data_clean + noise\n",
    "print('Shape of data: {}'.format(data_3.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create an instance of the Model class, using model_type='python', and we perform maximum likelihood estimation of the two parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_model = ComputationalModel(parameters_number=2, runmodel_object=h_func, error_covariance=error_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = MinimizeOptimizer(method='nelder-mead')\n",
    "ml_estimator = MLE(inference_model=candidate_model, data=data_3, optimizations_number=1)\n",
    "print('fitted parameters: theta_0={0:.3f} (true=1.), and theta_1={1:.3f} (true=2.)'.format(\n",
    "    ml_estimator.mle[0], ml_estimator.mle[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

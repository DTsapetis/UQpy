{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian parameter estimation using MCMC\n",
    "\n",
    "Authors: Audrey Olivier and Dimitris Giovanis <br>\n",
    "Last modified on 12/14/2018 by Audrey Olivier\n",
    "\n",
    "In Bayesian parameter estimation, one is looking to estimate the posterior pdf of the parameter vector characterizing the model, based on available data. The posterior pdf is given by Bayes' theorem:\n",
    "\n",
    "$$ p(\\theta \\vert data) = \\frac{p(data \\vert \\theta) p(\\theta)}{p(data)} $$\n",
    "\n",
    "The posterior pdf of nonlinear non-Gaussian models is often intractable, approximations must be used. In particular, Markov Chain Monte Carlo methods are a very popular way to sample from the posterior pdf. This notebook illustrates how to perfrom Bayesian parameter estimation via MCMC in UQpy, using the BayesParameterEstimation class. At the end of this notebook, we also show how to quickly check your results using some simple Diagnostics tools implemented in UQpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from UQpy.inference import DistributionModel, ComputationalModel, BayesParameterEstimation\n",
    "from UQpy.sampling import MhInput\n",
    "from UQpy.RunModel import RunModel  # required to run the quadratic model\n",
    "from sklearn.neighbors import KernelDensity  # for the plots\n",
    "from UQpy.distributions import JointIndependent, Uniform, Lognormal, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot posterior pdf from samples\n",
    "def pdf_from_kde(domain, samples1d):\n",
    "    bandwidth = 1.06 * np.std(samples1d) * samples1d.size ** (-1 / 5)\n",
    "    kde = KernelDensity(bandwidth=bandwidth).fit(samples1d.reshape((-1, 1)))\n",
    "    log_dens = kde.score_samples(domain)\n",
    "    return np.exp(log_dens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability model\n",
    "\n",
    "In the following we learn the mean and covariance of a univariate gaussian distribution from data.\n",
    "\n",
    "First, for the sake of this example, we generate fake data from a gaussian distribution with mean 10 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "mu, sigma = 10, 1  # true mean and standard deviation\n",
    "data_1 = np.random.normal(mu, sigma, 100).reshape((-1, 1))\n",
    "np.random.seed()\n",
    "\n",
    "# plot the data and true distribution\n",
    "count, bins, ignored = plt.hist(data_1, 30, density=True)\n",
    "plt.plot(bins, 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(- (bins - mu) ** 2 / (2 * sigma ** 2)),\n",
    "         linewidth=2, color='r')\n",
    "plt.title('data as histogram and true distribution to be estimated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Bayesian setting, the definition of a prior pdf is a key point. The prior for the parameters must be defined in the model. Note that if no prior is given, an inproper, uninformative, prior is chosen, $p(\\theta)=1$ for all $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = Uniform(loc=0., scale=15)\n",
    "p1 = Lognormal(s=1., loc=0., scale=1.)\n",
    "prior = JointIndependent(marginals=[p0, p1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of class Model\n",
    "candidate_model = DistributionModel(distributions=Normal(loc=None, scale=None), parameters_number=2, prior=prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the unknown parameters using MCMC\n",
    "mh_input = MhInput()\n",
    "mh_input.jump=10\n",
    "mh_input.burn_length=10\n",
    "mh_input.seed=np.array([1.0, 0.2])\n",
    "bayes_estimator = BayesParameterEstimation.create_with_mcmc_sampling(inference_model=candidate_model,\n",
    "                                                                     data=data_1,\n",
    "                                                                     samples_number=500,\n",
    "                                                                     mcmc_input=mh_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "s = bayes_estimator.sampler.samples\n",
    "plt.scatter(s[:, 0], s[:, 1])\n",
    "plt.scatter(10, 1, marker='+', label='true parameter')\n",
    "plt.title('MCMC samples')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "domain = np.linspace(0, 15, 200)[:, np.newaxis]\n",
    "pdf = pdf_from_kde(domain, s[:, 0])\n",
    "ax[0].plot(domain, p0.pdf(domain), label='prior')\n",
    "ax[0].plot(domain, pdf, label='posterior')\n",
    "ax[0].set_title('posterior pdf of theta=mu')\n",
    "ax[0].legend()\n",
    "\n",
    "domain = np.linspace(0, 2, 200)[:, np.newaxis]\n",
    "pdf = pdf_from_kde(domain, s[:, 1])\n",
    "ax[1].plot(domain, p1.pdf(domain), label='prior')\n",
    "ax[1].plot(domain, pdf, label='posterior')\n",
    "ax[1].set_title('posterior pdf of theta=sigma')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model\n",
    "\n",
    "Here a model is defined that is of the form \n",
    "\n",
    "$$y=f(\\theta) + \\epsilon$$\n",
    "\n",
    "where f consists in running RunModel. In particular, here $f(\\theta)=\\theta_{0} x + \\theta_{1} x^{2}$ is a regression model.\n",
    "\n",
    "First we generate synthetic data, and add some noise to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "param_true = np.array([1.0, 2.0]).reshape((1, -1))\n",
    "print('Shape of true parameter vector: {}'.format(param_true.shape))\n",
    "\n",
    "h_func = RunModel(model_script='pfn_models.py', model_object_name='model_quadratic', vec=False,\n",
    "                  var_names=['theta_0', 'theta_1'])\n",
    "h_func.run(samples=param_true)\n",
    "data_clean = np.array(h_func.qoi_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise, use a RandomState for reproducible results\n",
    "error_covariance = 1.\n",
    "noise = Normal(loc=0., scale=np.sqrt(error_covariance)).rvs(nsamples=50, random_state=123).reshape((50,))\n",
    "data_3 = data_clean + noise\n",
    "print('Shape of data: {}'.format(data_3.shape))\n",
    "print(data_3[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = Normal()\n",
    "p1 = Normal()\n",
    "prior = JointIndependent(marginals=[p0, p1])\n",
    "\n",
    "inference_model = ComputationalModel(parameters_number=2, runmodel_object=h_func, error_covariance=error_covariance,\n",
    "                                     prior=prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal = JointIndependent([Normal(scale=0.1), Normal(scale=0.05)])\n",
    "\n",
    "mh_input1= MhInput()\n",
    "mh_input1.jump=10\n",
    "mh_input1.burn_length=0\n",
    "mh_input1.proposal=proposal\n",
    "mh_input1.seed=[0.5, 2.5]\n",
    "mh_input1.random_state=456\n",
    "bayes_estimator = BayesParameterEstimation.create_with_mcmc_sampling(inference_model=inference_model,\n",
    "                                                                     data=data_3,\n",
    "                                                                     mcmc_input=mh_input1,\n",
    "                                                                     samples_number=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bayes_estimator.sampler.samples\n",
    "plt.scatter(s[:, 0], s[:, 1])\n",
    "plt.scatter(1.0, 2.0, label='true value')\n",
    "plt.title('MCMC samples')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "domain = np.linspace(-4, 4, 200)[:, np.newaxis]\n",
    "pdf_ = pdf_from_kde(domain, s[:, 0])\n",
    "ax[0].plot(domain, pdf_, label='posterior')\n",
    "ax[0].plot(domain, p0.pdf(domain), label='prior')\n",
    "ax[0].set_title('posterior pdf of theta_{1}')\n",
    "\n",
    "domain = np.linspace(-4, 4, 200)[:, np.newaxis]\n",
    "pdf_ = pdf_from_kde(domain, s[:, 1])\n",
    "ax[1].plot(domain, pdf_, label='posterior')\n",
    "ax[1].plot(domain, p1.pdf(domain), label='prior')\n",
    "ax[1].set_title('posterior pdf of theta_{2}')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(bayes_estimator.sampler.samples[:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

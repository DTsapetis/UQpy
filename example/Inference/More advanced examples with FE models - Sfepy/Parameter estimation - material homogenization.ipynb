{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter estimation in material homogenization\n",
    "\n",
    "This notebook illustrates performing parameter estimation tasks in UQpy for a finite element model. The problem is adapted from http://sfepy.org/doc-devel/mat_optim.html and uses the python package Sfepy to solve the finite element equations (Sfepy should be downloaded prior to running this example).\n",
    "\n",
    "This inference task consists in the identification of material parameters of a composite structure using data (force-displacement curves) obtained by a standard tensile test. The composite microstructure is shown below (see mesh plot for microstructure problem), and consists of linear elastic fibers randomly dispersed in a linear elastic matrix. The four parameters to be learnt from data are the young's moduli and poisson ratio of both the matrix and the fibers. The data consists in the slope of the force-displacement curves from four experiments (tensile tests of four specimen with different fiber orientations). Briefly, the homogenization equations are solved as follows:\n",
    "- equations for a representative volume of the microstructure are solved under periodic boundary conditions, yielding the stiffness matrix of the representative volume (first row of mesh plots below),\n",
    "- knowing the stiffness of a representative volume, one can solve the equations for the macro-problem, i.e., the specimen subjected to the tensile test (second row of mesh plots below).\n",
    "\n",
    "### Illustration of one simulation run\n",
    "\n",
    "Codes are adapted from http://sfepy.org/doc-devel/mat_optim.html, the main file material_homogenization.py calls functions and data files from the package Sfepy. The following cells show how to run one simulation, for a given parameter value. Alternatively, one could also use RunModel, which will be used later when creating a model and running Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Model' from 'UQpy.Inference' (C:\\Users\\dimtsap\\Anaconda3\\lib\\site-packages\\UQpy\\Inference.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3ad80f794e70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmaterial_homogenization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mUQpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInference\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMLEstimation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBayesParameterEstimation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mUQpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Model' from 'UQpy.Inference' (C:\\Users\\dimtsap\\Anaconda3\\lib\\site-packages\\UQpy\\Inference.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from material_homogenization import *\n",
    "from UQpy.Inference import Model, MLEstimation, BayesParameterEstimation\n",
    "from UQpy.Distributions import Distribution\n",
    "\n",
    "# Define data for maximization algorithms as in http://sfepy.org/doc-devel/mat_optim.html.\n",
    "data = np.array([1051140., 197330., 101226., 95474.])\n",
    "var_names = ['E_fiber', 'v_fiber', 'E_matrix', 'v_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([160.e9, 0.25, 5.e9, 0.45])\n",
    "print('E_fiber = {} GPa, v_fiber = {}, E_matrix = {} GPa, v_matrix = {}'.format(x0[0]/1e9, x0[1], x0[2]/1e9, x0[3]))\n",
    "x0 = x_real2norm(x0)\n",
    "qoi = one_simulation(x0, plot_meshes_bool=True)\n",
    "print('Computed slopes of the force-elongation tangent lines for fiber orientations 0, 30, 60 and 90 degrees')\n",
    "print(qoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximization as performed in original code presented in Sfepy\n",
    "\n",
    "See http://sfepy.org/doc-devel/mat_optim.html. The function to be minimized is \n",
    "$$ \\Phi = \\sum_{\\phi=0, 30, 60, 90} \\left( 1-\\frac{k_{computed, \\phi}}{k_{experiment, \\phi}} \\right)^{2} $$\n",
    "The identified parameters in this reference are: E_f=171 GPa, v_f=0.32, E_m=2.33 GPa, v_m = 0.20, but the authors note that the results may vary across SciPy versions and related libraries.\n",
    "\n",
    "The parameters are defined over well-defined bounds, see x_L and x_U in material_homogenization.py. It is often best practice to scale the parameters so that they have comparable orders of magnitude, thus in the following the parameters are scaled so that they evolve in the $[0, 1]$ bounds. The functions x_real2norm and x_norm2real perform this scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximization function, as in http://sfepy.org/doc-devel/mat_optim.html.\n",
    "def func(x0, exp_k):\n",
    "    comp_k = one_simulation(x0)\n",
    "    \n",
    "    val = 0.0\n",
    "    for e_k, c_k in zip(exp_k, comp_k):\n",
    "        val += (1.0 - c_k / e_k)**2\n",
    "    #val = np.sqrt(val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "x0 = x_real2norm([160.e9, 0.25, 5.e9, 0.45])\n",
    "xopt = minimize(func, x0, args=(data,), bounds=[(0, 1) for _ in range(4)], method = 'SLSQP')\n",
    "print('number of function evaluations required for optimization: '.format(xopt.nit))\n",
    "xfinal = x_norm2real(xopt.x)\n",
    "print('Results of optimization procedure:')\n",
    "print('E_fiber = {} GPa, v_fiber = {}, E_matrix = {} GPa, v_matrix = {}'.\n",
    "      format(xfinal[0]/1e9, xfinal[1], xfinal[2]/1e9, xfinal[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood with UQpy\n",
    "\n",
    "Recall that maximizing the likelihood of a model $y_{\\phi}=f_{\\phi}(\\theta)+\\varepsilon$, where $\\varepsilon \\sim N(\\cdot; 0, \\sigma_{\\phi}^2)$ is equivalent to minimizing the weighted sum of squares $\\frac{(y_{\\phi}-f_{\\phi}(\\theta))^2}{2\\sigma_{\\phi}^2}$. Thus the above maximization can be performed using UQpy maximum likelihood estimator, setting $y_{\\phi}=k_{experiment, \\phi}, f_{\\phi}(\\theta)=k_{computed, \\phi}, \\sigma_{\\phi}^2=\\frac{1}{2} k_{experiment, \\phi}^2$. This is done in the following by setting the error_covariance input accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = Model(n_params=4, model_type='python', \n",
    "              model_script='material_homogenization.py', model_object_name = 'one_simulation',\n",
    "              error_covariance=1/2*data**2, var_names = var_names)\n",
    "\n",
    "# Maximum likelihood with weighted variance\n",
    "ml_estimator = MLEstimation(model=model, data=data, bounds=[(0, 1) for _ in range(4)],\n",
    "                            x0=x_real2norm([160.e9, 0.25, 5.e9, 0.45]))\n",
    "xfinal = x_norm2real(ml_estimator.param)\n",
    "print('Results of ML procedure:')\n",
    "print('E_fiber = {} GPa, v_fiber = {}, E_matrix = {} GPa, v_matrix = {}'.\n",
    "      format(xfinal[0]/1e9, xfinal[1], xfinal[2]/1e9, xfinal[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a model in UQPy, it is possible to plot the log-likelihood function as a function of the parameters. In the following cell we fix three parameters $\\theta_{i1, i2, i3}$ out of the four unknown parameters and plot the conditional log-likelihood as we vary the fourth parameter over its range. In the following cell, we plot the variation of the log-likelihood as we vary two parameters.\n",
    "\n",
    "Such analysis helps in understanding which parameters most affect the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_params=4, model_type='python', \n",
    "              model_script='material_homogenization.py', model_object_name = 'one_simulation',\n",
    "              error_covariance=1/2*data**2, var_names = ['E_f', 'v_f', 'E_m', 'v_m'])\n",
    "\n",
    "# Look at the likelihood surfaces in 2D, fix the remaining two parameters\n",
    "fixed_p = x_real2norm([161.79e9, 0.3490, 2.3207e9, 0.20])\n",
    "\n",
    "print('Conditional log likelihood, when three parameters are fixed to their max likelihood value.')\n",
    "npoints = 20\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(16,3.5))\n",
    "for j in range(4):\n",
    "    xx = np.linspace(0, 1, npoints)\n",
    "    samples = fixed_p.reshape((1,4))*np.ones((npoints,4))\n",
    "    samples[:,j] = xx\n",
    "    tmp = np.array([x_norm2real(s) for s in samples])\n",
    "    zz = model.log_like(data, samples)\n",
    "    ax[j].plot(tmp[:,j], zz)\n",
    "    ax[j].set_xlabel(var_names[j])\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Conditional log likelihood, when two parameters are fixed to their max likelihood value.')\n",
    "npoints = 8\n",
    "vars_d2 = [[0, 1], [2, 3], [0, 2], [1, 3]]\n",
    "axs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(14,8))\n",
    "for j, var_d2 in enumerate(vars_d2):\n",
    "    x = np.linspace(0, 1, npoints)\n",
    "    y = np.linspace(0, 1, npoints)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    xx, yy = xx.reshape((-1,)), yy.reshape((-1,))\n",
    "    samples = fixed_p.reshape((1,4))*np.ones((npoints**2,4))\n",
    "    samples[:,var_d2[0]] = xx\n",
    "    samples[:,var_d2[1]] = yy\n",
    "    zz = model.log_like(data, samples)\n",
    "    tmp = np.array([x_norm2real(s) for s in samples])\n",
    "    x, y = tmp[:,var_d2[0]], tmp[:,var_d2[1]]\n",
    "    ax_j = ax[axs[j][0],axs[j][1]]\n",
    "    t = ax_j.contourf(x.reshape((npoints,npoints)),y.reshape((npoints,npoints)),zz.reshape((npoints,npoints)), 20)\n",
    "    ax_j.set_title('Conditional log likelihood')\n",
    "    ax_j.set_xlabel(var_names[var_d2[0]])\n",
    "    ax_j.set_ylabel(var_names[var_d2[1]])\n",
    "    cbar = plt.colorbar(t, ax=ax_j)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pdf estimation with UQpy\n",
    "\n",
    "Run MCMC on the problem defined above. When performing a Bayesian analysis, it is important to carefully define both the prior of the parameters and the error_covariance of the model. Here the prior is chosen uniform over [0, 1], the error_covariance as $1/100*data**2$, meaning that the error in the measurements is proportional to the measured value, with a coefficient of variation of 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = Model(n_params=4, model_type='python', \n",
    "              model_script='material_homogenization.py', model_object_name = 'one_simulation',\n",
    "              error_covariance=1/100*data**2, var_names = ['E_f', 'v_f', 'E_m', 'v_m'],\n",
    "              ntasks=4, prior_name=['uniform']*4, prior_params=[[0,1]]*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several small MCMC runs were performed to try and find the best scale parameters for this problem. Note that it may take about 30 minutes to run MCMC with 300 samples. The code to run to perform parameter estimation with MCMC is shown in the following cell. Results of several runs were saved in a file called 'mcmc_results_last.pkl'. The following scale parameters were tried:\n",
    "- mcmc_0: [0.1, 0.1, 0.02, 0.05]\n",
    "- mcmc_1: [0.15, 0.2, 0.02, 0.1]\n",
    "- mcmc_2: [0.2, 0.25, 0.04, 0.1]\n",
    "- mcmc_3: [0.2, 0.25, 0.2, 0.2]\n",
    "- mcmc_4: [0.2, 0.25, 0.04, 0.1], 5000 samples\n",
    "- mcmc_4: [0.2, 0.25, 0.04, 0.1], 10000 samples <br>\n",
    "In the following we show the effect of modifying the scale parameter on MCMC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This would be the code to run to perform parameter estimation using MCMC\n",
    "#be = BayesParameterEstimation(data=data, model=model, sampling_method = 'MCMC', nsamples=300,\n",
    "#                              algorithm = 'MH', jump=1, nburn=0, pdf_proposal_type = 'Normal',\n",
    "#                              pdf_proposal_scale = [0.1, 0.1, 0.02, 0.05], \n",
    "#                              seed = x_real2norm([162e9, 0.35, 2.32e9, 0.25]))\n",
    "## Save the data into a pickle file\n",
    "#import pickle\n",
    "#with open('mcmc_results_test.pkl', 'wb') as f:\n",
    "#    pickle.dump({'mcmc_0': be}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('mcmc_results_last.pkl', 'rb') as f:\n",
    "    mcmc_results = pickle.load(f)\n",
    "l = ['mcmc_3', 'mcmc_0', 'mcmc_2', 'mcmc_4']\n",
    "to_print = ['scale parameter is too large, acceptance ratio too small:', 'scale_parameter is too small:', \n",
    "            'good scale - 300 samples:', 'good scale - 5000 samples:']\n",
    "for j, l_j in enumerate(l):\n",
    "    be = mcmc_results[l_j]\n",
    "    print('\\n'+to_print[j])\n",
    "    print('acceptance ratio: {}'.format(be.accept_ratio))\n",
    "    print('last_sample: {}'.format(be.samples[-1,:]))\n",
    "    print('sample std: {}'.format(np.std(be.samples, axis=0)))\n",
    "    fig, ax = plt.subplots(ncols=4, figsize=(20,3))\n",
    "    tmp = np.array([x_norm2real(s) for s in be.samples])\n",
    "    for i, param_name in enumerate(var_names):\n",
    "        ax[i].plot(tmp[:,i])\n",
    "        ax[i].set_title(param_name)\n",
    "        ax[i].set_ylabel('Markov Chain')\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the previous 5000 samples to plot the posterior pdfs. We take out a few samples at the beginning, since the seed was given as the maximum likelihood estimate we can assume that the burn-in period would be small in this example. We also keep only 1 out of 5 samples to avoid for them to be correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nburn = 200\n",
    "jump = 4\n",
    "\n",
    "be = mcmc_results['mcmc_5']\n",
    "samples = be.samples[nburn::jump]\n",
    "print(samples.shape)\n",
    "samples = np.array([x_norm2real(x) for x in samples])\n",
    "x_L = x_norm2real([0, 0, 0, 0])\n",
    "x_U = x_norm2real([1, 1, 1, 1])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(22,3.5))\n",
    "for i, param_name in enumerate(var_names):\n",
    "    ax[i].hist(samples[:,i], density=True, bins=30)\n",
    "    ax[i].set_title(param_name)\n",
    "    ax[i].set_ylabel('posterior pdf')\n",
    "    ax[i].set_xlim([x_L[i], x_U[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UQpy.Utilities import diagnostics\n",
    "diagnostics(sampling_method='MCMC', samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostics for MCMC methods\n",
    "\n",
    "This notebook illustrates the use of a few diagnostics for MCMC. The example consists in learning the parameters of a regression model via Bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UQpy.sampling import MetropolisHastings\n",
    "from UQpy.distributions import Normal, JointIndependent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example setup\n",
    "\n",
    "First generate data then define the target log pdf = log likelihood $p(data \\vert x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_true = [1., 2.]\n",
    "domain = np.linspace(0, 10, 50)\n",
    "data_clean = param_true[0] * domain + param_true[1] * domain ** 2\n",
    "rstate = np.random.RandomState(123)\n",
    "data_noisy = data_clean + rstate.randn(*data_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def log_target(x, data, x_domain):\n",
    "    log_target_value = np.zeros(x.shape[0])\n",
    "    for i, xx in enumerate(x):\n",
    "        h_xx = xx[0] * x_domain + xx[1] * x_domain ** 2\n",
    "        log_target_value[i] = np.sum([norm.logpdf(hxi - datai) for hxi, datai in zip(h_xx, data)])\n",
    "    return log_target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal = JointIndependent([Normal(scale=0.1), Normal(scale=0.05)])\n",
    "\n",
    "from UQpy.sampling.input_data import MhInput\n",
    "\n",
    "mh_input = MhInput(dimension=2, log_pdf_target=log_target, burn_length=10, jump=10, chains_number=1,\n",
    "                  args_target=(data_noisy, domain), proposal=proposal)\n",
    "\n",
    "sampler = MetropolisHastings(mh_input=mh_input, samples_number=500)\n",
    "\n",
    "print(sampler.samples.shape)\n",
    "samples = sampler.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples[:, 0], samples[:, 1], 'o', alpha=0.5)\n",
    "plt.plot(1., 2., marker='x', color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at acceptance rate of the chain(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampler.acceptance_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This acceptance rate is quite low, showing that the proposal should probably be refined !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15, 7))\n",
    "for j in range(samples.shape[1]):\n",
    "    ax[j, 0].plot(np.arange(samples.shape[0]), samples[:, j])\n",
    "    ax[j, 0].set_title('chain - parameter # {}'.format(j+1))\n",
    "    ax[j, 1].plot(np.arange(samples.shape[0]), np.cumsum(samples[:, j])/np.arange(samples.shape[0]))\n",
    "    ax[j, 1].set_title('parameter convergence')\n",
    "    ax[j, 2].acorr(samples[:, j] - np.mean(samples[:, j]), maxlags=40, normed=True)\n",
    "    ax[j, 2].set_title('correlation between samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute effective sample size ESS\n",
    "\n",
    "In MCMC, the $ESS$ has been used to derive termination rules, based on the quality of the estimation. In brief, the simulation stops when the computational uncertainty on a chosen quantity (for instance, the expected value of RV $x$) is small compared to its posterior uncertainty [1]. Mathematically, this allows computation of the $ESS$ and a minimum value $ESS_{min}$ (qualitatively, an acceptable approximation is reached if $ESS > ESS_{min}$). These quantities can be computed by looking at each marginal density (is $x$ is a multivariate random variable), or by looking at the joint. The reader is referred to [1, 2] for more details.\n",
    "\n",
    "[1] *A practical sequential stopping rule for high-dimensional MCMC and its application to spatial-temporal Bayesian models*, Gong and Flegal, 2014. <br>\n",
    "[2] *Multivariate Output Analysis for Markov Chain Monte Carlo*, Vats and Flegal, 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_marginal_ESS(samples, eps_ESS=0.05, alpha_ESS=0.05):\n",
    "    # Computation of ESS and min ESS, look at each marginal distribution separately\n",
    "    nsamples, dim = samples.shape\n",
    "    \n",
    "    bn = np.ceil(nsamples**(1/2))    # nb of samples per bin\n",
    "    an = int(np.ceil(nsamples/bn))    # nb of bins\n",
    "    idx = np.array_split(np.arange(nsamples), an)\n",
    "\n",
    "    means_subdivisions = np.empty((an, samples.shape[1]))\n",
    "    for i, idx_i in enumerate(idx):\n",
    "        x_sub = samples[idx_i, :]\n",
    "        means_subdivisions[i, :] = np.mean(x_sub, axis=0)\n",
    "    Omega = np.cov(samples.T)\n",
    "    Sigma = np.cov(means_subdivisions.T)\n",
    "\n",
    "    marginal_ESS = np.empty((dim, ))\n",
    "    min_marginal_ESS = np.empty((dim,))\n",
    "    for j in range(dim):\n",
    "        marginal_ESS[j] = nsamples * Omega[j,j] / Sigma[j,j]\n",
    "        min_marginal_ESS[j] = 4 * norm.ppf(alpha_ESS/2)**2 / eps_ESS**2\n",
    "        \n",
    "    return marginal_ESS, min_marginal_ESS\n",
    "\n",
    "def compute_joint_ESS(samples, eps_ESS=0.05, alpha_ESS=0.05):\n",
    "    # Computation of ESS and min ESS, look at the joint distribution separately\n",
    "    nsamples, dim = samples.shape\n",
    "    \n",
    "    bn = np.ceil(nsamples**(1/2))    # nb of samples per bin\n",
    "    an = int(np.ceil(nsamples/bn))    # nb of bins\n",
    "    idx = np.array_split(np.arange(nsamples), an)\n",
    "\n",
    "    means_subdivisions = np.empty((an, samples.shape[1]))\n",
    "    for i, idx_i in enumerate(idx):\n",
    "        x_sub = samples[idx_i, :]\n",
    "        means_subdivisions[i, :] = np.mean(x_sub, axis=0)\n",
    "    Omega = np.cov(samples.T)\n",
    "    Sigma = np.cov(means_subdivisions.T)\n",
    "    \n",
    "    from scipy.stats import chi2\n",
    "    from scipy.special import gamma\n",
    "    joint_ESS = nsamples*np.linalg.det(Omega)**(1/dim)/np.linalg.det(Sigma)**(1/dim)\n",
    "    chi2_value = chi2.ppf(1 - alpha_ESS, df=dim)\n",
    "    min_joint_ESS = 2 ** (2 / dim) * np.pi / (dim * gamma(dim / 2)) ** (\n",
    "                2 / dim) * chi2_value / eps_ESS ** 2\n",
    "    \n",
    "    return joint_ESS, min_joint_ESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_ESS, min_marginal_ESS = compute_marginal_ESS(samples, eps_ESS=0.05, alpha_ESS=0.05)\n",
    "\n",
    "print('Univariate Effective Sample Size in each dimension:')\n",
    "for j in range(2):\n",
    "    print('Dimension {}: ESS = {}, minimum ESS recommended = {}'.\n",
    "          format(j+1, marginal_ESS[j], min_marginal_ESS[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_ESS, min_joint_ESS = compute_joint_ESS(samples, eps_ESS=0.05, alpha_ESS=0.05)\n",
    "\n",
    "print('\\nMultivariate Effective Sample Size:')\n",
    "print('Multivariate ESS = {}, minimum ESS recommended = {}'.format(joint_ESS, min_joint_ESS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results abobe show that the chains should probably be run for a longer amount of time in order to obtain reliable results in terms of the expected value of the parameters to be learnt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
